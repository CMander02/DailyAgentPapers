"""
Markdown æ–‡ä»¶ç”Ÿæˆå™¨
- ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„ markdown æ–‡ä»¶ (YAML frontmatter)
- æ›´æ–° README.md
- ç”Ÿæˆ papers.json ä¾›å‰ç«¯ä½¿ç”¨
"""

import os
import re
import json
from datetime import datetime
from typing import Optional


def slugify(title: str) -> str:
    """å°†æ ‡é¢˜è½¬ä¸ºæ–‡ä»¶åå‹å¥½çš„ slug"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
    slug = re.sub(r"[^\w\s-]", "", title.lower())
    slug = re.sub(r"[\s]+", "-", slug.strip())
    slug = re.sub(r"-+", "-", slug)
    # é™åˆ¶é•¿åº¦
    return slug[:80].rstrip("-")


def generate_paper_markdown(paper: dict, llm_result: dict) -> str:
    """ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„ markdown å†…å®¹"""
    # YAML frontmatter
    authors_list = []
    for a in paper["authors"][:20]:
        author_str = a["name"]
        if a.get("affiliation"):
            author_str += f" ({a['affiliation']})"
        # Escape quotes in YAML
        author_str = author_str.replace('"', '\\"')
        authors_list.append(f'  - "{author_str}"')

    tags_list = [f'  - "{t}"' for t in llm_result.get("tags", [])]
    cats_list = [f'  - "{c}"' for c in paper.get("categories", [])]

    contributions = llm_result.get("core_contributions", [])
    contributions_md = "\n".join(f"- {c}" for c in contributions)

    escaped_title = paper['title'].replace('"', '\\"')

    md = f"""---
title: "{escaped_title}"
authors:
{chr(10).join(authors_list)}
date: "{paper['published'][:10]}"
arxiv_id: "{paper['arxiv_id']}"
arxiv_url: "{paper['arxiv_url']}"
pdf_url: "{paper['pdf_url']}"
categories:
{chr(10).join(cats_list)}
tags:
{chr(10).join(tags_list)}
relevance_score: {llm_result.get('relevance_score', 0)}
---

# {paper['title']}

## åŸå§‹æ‘˜è¦

{paper['summary']}

## ä¸­æ–‡æ‘˜è¦

{llm_result.get('chinese_summary', 'N/A')}

## æ ¸å¿ƒè´¡çŒ®

{contributions_md}

## æ–‡ç« è§£è¯»

{llm_result.get('analysis', 'N/A')}
"""
    return md


def write_paper_file(paper: dict, llm_result: dict, base_dir: str, date_str: str) -> str:
    """
    å†™å…¥å•ç¯‡è®ºæ–‡çš„ markdown æ–‡ä»¶ã€‚
    è¿”å›æ–‡ä»¶è·¯å¾„ã€‚
    """
    # date_str: YYYY-MM-DD â†’ YYYY/MM/DD
    year, month, day = date_str.split("-")
    dir_path = os.path.join(base_dir, "data", year, month, day)
    os.makedirs(dir_path, exist_ok=True)

    slug = slugify(paper["title"])
    filename = f"{slug}.md"
    filepath = os.path.join(dir_path, filename)

    content = generate_paper_markdown(paper, llm_result)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)

    return filepath


def update_readme(papers_with_results: list[tuple[dict, dict]], base_dir: str, date_str: str):
    """æ›´æ–° README.mdï¼Œå±•ç¤ºå½“æ—¥æœ€æ–°è®ºæ–‡"""
    readme_path = os.path.join(base_dir, "README.md")

    # æŒ‰è¯„åˆ†æ’åº
    sorted_papers = sorted(
        papers_with_results,
        key=lambda x: x[1].get("relevance_score", 0),
        reverse=True,
    )

    year, month, day = date_str.split("-")

    lines = []
    lines.append("# DailyAgentPapers")
    lines.append("")
    lines.append("æ¯æ—¥ Arxiv Agent è®ºæ–‡è‡ªåŠ¨æ‘˜è¦ | Daily Arxiv Agent Paper Summaries")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append(f"## ğŸ“‹ æœ€æ–°è®ºæ–‡ | {date_str}")
    lines.append("")
    lines.append(f"å…±æ”¶å½• **{len(sorted_papers)}** ç¯‡ Agent ç›¸å…³è®ºæ–‡")
    lines.append("")

    for paper, result in sorted_papers:
        score = result.get("relevance_score", 0)
        tags = result.get("tags", [])
        tags_str = " ".join(f"`{t}`" for t in tags[:5])
        slug = slugify(paper["title"])

        lines.append(f"### [{paper['title']}]({paper['arxiv_url']})")
        lines.append("")
        lines.append(f"**è¯„åˆ†: {score}/10** | {tags_str}")
        lines.append("")
        lines.append(f"> {result.get('chinese_summary', 'N/A')[:200]}...")
        lines.append("")
        lines.append(
            f"ğŸ“„ [è¯¦ç»†è§£è¯»](data/{year}/{month}/{day}/{slug}.md) | "
            f"ğŸ“ [PDF]({paper['pdf_url']})"
        )
        lines.append("")
        lines.append("---")
        lines.append("")

    lines.append("## ğŸ“… å†å²å½’æ¡£")
    lines.append("")
    lines.append("è®ºæ–‡æŒ‰æ—¥æœŸå½’æ¡£åœ¨ `data/YYYY/MM/DD/` ç›®å½•ä¸‹ã€‚")
    lines.append("")
    lines.append("## ğŸ”§ å…³äº")
    lines.append("")
    lines.append("æœ¬é¡¹ç›®é€šè¿‡ GitHub Actions æ¯æ—¥è‡ªåŠ¨è¿è¡Œï¼Œä½¿ç”¨ arxiv API è·å–è®ºæ–‡ï¼Œ")
    lines.append("LLM è¿›è¡Œæ™ºèƒ½ç­›é€‰å’Œä¸­æ–‡æ‘˜è¦ç”Ÿæˆã€‚")
    lines.append("")
    lines.append("- æ•°æ®æº: [arxiv.org](https://arxiv.org/)")
    lines.append("- å…³æ³¨é¢†åŸŸ: AI Agent, Multi-Agent Systems, LLM Agent, Tool Use, Planning, Reasoning")
    lines.append("- æ›´æ–°é¢‘ç‡: æ¯æ—¥åŒ—äº¬æ—¶é—´ 07:00")
    lines.append("")

    with open(readme_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


def update_papers_json(papers_with_results: list[tuple[dict, dict]], base_dir: str, date_str: str):
    """
    æ›´æ–° papers.json ä¾›å‰ç«¯ GitHub Pages ä½¿ç”¨ã€‚
    ç»“æ„: {dates: {YYYY-MM-DD: [paper_summary, ...]}}
    """
    json_path = os.path.join(base_dir, "data", "papers.json")

    # åŠ è½½å·²æœ‰æ•°æ®
    if os.path.exists(json_path):
        with open(json_path, "r", encoding="utf-8") as f:
            all_data = json.load(f)
    else:
        all_data = {"dates": {}}

    # æ„å»ºå½“æ—¥æ•°æ®
    day_papers = []
    for paper, result in papers_with_results:
        year, month, day = date_str.split("-")
        slug = slugify(paper["title"])
        day_papers.append({
            "arxiv_id": paper["arxiv_id"],
            "title": paper["title"],
            "authors": [a["name"] for a in paper["authors"][:5]],
            "author_count": len(paper["authors"]),
            "categories": paper["categories"],
            "arxiv_url": paper["arxiv_url"],
            "pdf_url": paper["pdf_url"],
            "published": paper["published"][:10],
            "tags": result.get("tags", []),
            "relevance_score": result.get("relevance_score", 0),
            "chinese_summary": result.get("chinese_summary", ""),
            "core_contributions": result.get("core_contributions", []),
            "analysis": result.get("analysis", ""),
            "md_path": f"data/{year}/{month}/{day}/{slug}.md",
        })

    # æŒ‰è¯„åˆ†æ’åº
    day_papers.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)

    all_data["dates"][date_str] = day_papers

    # ä¿ç•™æœ€è¿‘ 90 å¤©æ•°æ®ï¼Œé˜²æ­¢æ–‡ä»¶è¿‡å¤§
    sorted_dates = sorted(all_data["dates"].keys(), reverse=True)
    if len(sorted_dates) > 90:
        for old_date in sorted_dates[90:]:
            del all_data["dates"][old_date]

    all_data["available_dates"] = sorted(all_data["dates"].keys(), reverse=True)

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
