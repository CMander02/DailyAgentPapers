---
title: "Pipeline for Verifying LLM-Generated Mathematical Solutions"
authors:
  - "Varvara Sazonova"
  - "Dmitri Shmelkin"
  - "Stanislav Kikot"
  - "Vasily Motolygin"
date: "2026-02-24"
arxiv_id: "2602.20770"
arxiv_url: "https://arxiv.org/abs/2602.20770"
pdf_url: "https://arxiv.org/pdf/2602.20770v1"
categories:
  - "cs.AI"
tags:
  - "Agent 评测/基准"
  - "数学推理"
  - "形式化验证"
  - "多智能体系统"
  - "工具使用"
relevance_score: 7.5
---

# Pipeline for Verifying LLM-Generated Mathematical Solutions

## 原始摘要

With the growing popularity of Large Reasoning Models and their results in solving mathematical problems, it becomes crucial to measure their capabilities. We introduce a pipeline for both automatic and interactive verification as a more accurate alternative to only checking the answer which is currently the most popular approach for benchmarks. The pipeline can also be used as a generator of correct solutions both in formal and informal languages. 3 AI agents, which can be chosen for the benchmark accordingly, are included in the structure. The key idea is the use of prompts to obtain the solution in the specific form which allows for easier verification using proof assistants and possible use of small models ($\le 8B$). Experiments on several datasets suggest low probability of False Positives. The open-source implementation with instructions on setting up a server is available at https://github.com/LogicEnj/lean4_verification_pipeline.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决当前大语言模型（LLMs）在数学问题求解能力评估中存在的核心缺陷。随着大语言模型在解决数学问题上的应用日益广泛，准确衡量其推理能力变得至关重要。目前的主流评估方法（如常见基准测试）通常仅检查最终答案的正确性，但这种方法存在严重不足：许多复杂的、面向人类的数学问题，其核心价值在于严谨、完整的推理过程，而非仅仅一个答案。现有方法容易导致模型“猜测”答案，而回避或生成难以追踪的推理步骤，这无法真实反映模型的深层推理能力，也使得那些答案未知或涉及小众领域的问题更难被可靠评估。

针对这些不足，本文提出并构建了一个用于验证LLM生成的数学解决方案的自动化流水线。其核心目标是超越简单的答案匹配，实现对解决方案**推理过程质量**的自动且可靠的评估。该流水线通过结构化提示引导模型生成特定格式的解决方案，并利用证明助手（如Lean）来严格验证推理步骤的正确性，从而显著降低误判（False Positives）的概率。它不仅能生成评估报告，还能作为正确解决方案（包括形式化与非形式化语言）的生成器。通过引入求解器、翻译器和证明器三个可定制的AI智能体角色，该流水线提供了一种更准确、可解释的评估框架，以弥补现有仅基于答案的评估方法的重大短板。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为三类：验证与评测方法、神经定理证明以及数据集与模型改进。

在验证与评测方法方面，相关工作较少。Zheng等人提供了用于验证的基准数据集，而OpenAI等工作则利用人工标注的数据集来提升模型生成证明步骤的正确性。这些工作侧重于非形式化验证或概率性评估，无法提供形式化保证。本文提出的流水线则通过结合证明助手（如Lean4）进行自动和交互式验证，旨在对自然语言生成的解决方案提供形式化的正确性保证，这与仅检查最终答案或依赖概率的现有基准方法有本质区别。

在神经定理证明领域，研究重点是利用模型进行自动形式化（将非形式数学文本转化为形式语言）或在形式语言中生成证明，例如LeanAide、Lean Copilot等工具，以及DeepSeek-Prover、AlphaProof等专用模型。这些工作的目标是生成正确的形式化证明，而本文的目标是验证给定的自然语言解决方案是否正确。因此，本文的流水线将神经定理证明中的工具（如自动形式化和证明生成）作为关键组件整合进来，但服务于不同的最终目的——验证而非生成。

在方法策略上，许多先进模型（如Kimina、Aristotle）采用“分而治之”的策略，将证明分解为更易处理的引理。与本文最接近的是“Draft, Sketch, Prove”方法，它首先生成易于形式化的证明草图，再由LLM完善。本文借鉴了“草图”这一核心思想，但对其进行了重构，使其适用于LLM验证任务，而非纯粹的神经定理证明。

### Q3: 论文如何解决这个问题？

该论文通过设计一个结合大型语言模型与证明助手的自动化验证流水线来解决LLM生成数学解决方案的验证问题。其核心方法是强制要求Solver LLM生成具有特定结构的解决方案，该结构便于后续的形式化验证，并利用三个专门的AI代理分步协作完成验证。

整体框架包含三个主要模块：Solver LLM负责根据问题描述和特定提示词生成结构化的自然语言证明草稿；Translator LLM负责将证明草稿中的“事实”和引理自动形式化为Lean4代码；Prover LLM则负责在Lean4中证明这些形式化后的引理。整个算法流程分为六个步骤：1) 获取结构化解决方案；2) 用脚本分析解决方案结构并进行清洗与重写；3) 形式化所有“事实”并验证其证据；4) 形式化所有引理和定理，并用脚本检查翻译质量；5) 证明所有引理；6) 将整个证明链接起来。

关键技术在于通过提示工程约束解决方案的结构。要求证明必须由一系列逻辑步骤（引理）组成，每个引理以命题逻辑形式写出，包含一组前提和一个单一结论。前提只能是问题陈述、已知事实或先前引理的结论。这种结构被形式化为一个清晰的蕴含链，使得翻译、证明和链接过程更不易出错，并且便于用户交互验证。另一个创新点是处理文本问题的“变量引入”选项，通过特定提示要求Solver LLM明确声明并固定解决方案中使用的变量及其类型，从而避免因变量类型模糊导致的形式化困难。

此外，流水线支持自动和交互两种模式。在自动模式下，任何步骤失败都会导致算法终止并返回否定答案。在交互模式下，用户可以在关键步骤（如形式化错误或证明失败时）进行干预，例如提供正确翻译、跳过某个事实或指示重试，这提高了流水线的灵活性和实用性。论文通过使用较小的模型（如Qwen3-8B, Kimina系列）和辅助脚本，在降低资源消耗的同时，实现了较低的误报率。

### Q4: 论文做了哪些实验？

论文的实验设置基于一个包含三个AI智能体的自动化与交互式验证流程，用于评估大语言模型生成的数学解决方案。实验主要使用了Math-500数据集及其子集，包括：整个Math-500数据集（含500个问题，但排除了组合类等当前难以形式化的问题）、"easy"子集（10个简单问题，用于测试误报情况）和"similar"子集（150个问题，排除了几乎无法形式化或证明的问题）。对比方法主要涉及自动模式与交互模式下的验证效果，其中交互模式依赖用户对Lean4的掌握程度。

主要结果方面，在自动模式下，对于"easy"数据集，准确率（Accuracy）为0.840 ± 0.102，精确率（Precision）为0.822 ± 0.113，召回率（Recall）为0.953 ± 0.058；对于"similar"数据集，准确率为0.847 ± 0.016，精确率达到0.984 ± 0.008（接近1），召回率为0.609 ± 0.006（可通过脚本和更大模型改进）。在整个Math-500数据集上，精确率为0.942 ± 0.008。实验表明，该流程在排除几何、高等数学等当前无法处理的类别后，能有效生成正确解，且误报（False Positive）概率极低，但召回率仍有提升空间。

### Q5: 有什么可以进一步探索的点？

该论文提出的验证管道虽然有效，但仍存在一些局限性和可进一步探索的方向。首先，其验证过程高度依赖形式化证明助手（如Lean4），而数学严谨性对人类和证明助手标准不同，导致许多“假阴性”源于类型问题或对人类而言微小、对机器却致命的步骤缺失。这提示未来研究需开发更智能的ML模型或脚本，以系统性地弥合人类推理与形式化验证之间的鸿沟。

其次，管道采用多智能体链式结构，任一环节的错误都会导致整个验证失败。虽然目前难以完全避免，但未来可探索更鲁棒的容错机制，例如引入多个智能体并行验证或设计交叉检查流程，以减少单点故障风险。此外，当前方法对难题（如奥数或特定领域问题）的泛化能力有限，因为复杂问题难以分解为“简单/显然”的引理。这需要开发新的方法论，或许可结合领域知识库或分层抽象技术。

另一个重要方向是提升上下文保持能力与处理非等价数学符号的能力。当前依赖脚本处理，但未来可通过改进提示工程或训练专用模型来更自然地维持推理连贯性。最后，论文中对比的“仅检查答案”的管道在部分任务上表现更优，这启示我们可探索混合策略，结合答案验证与详细解验证的优势，以适应不同应用场景（如快速批阅与深度纠错）。这些改进有望增强管道在复杂、开放域数学问题上的实用性与可靠性。

### Q6: 总结一下论文的主要内容

该论文针对当前大语言模型在解决数学问题时仅通过最终答案进行评测的局限性，提出了一种用于验证LLM生成数学解决方案的自动化与交互式验证流程。其核心贡献在于设计了一个包含三个可替换AI代理的框架，通过特定的提示工程，将模型的解答过程转化为便于形式化验证（如使用Lean等证明助手）的结构化形式，从而实现对解题步骤而不仅仅是最终答案的严格检验。该方法不仅能更准确地评估模型的数学推理能力，还能作为生成正确形式化或非形式化解题过程的工具。实验表明，该流程产生误报（False Positives）的概率极低，适用于生成可靠的解决方案。论文的意义在于为数学推理模型的评估提供了更严谨、可解释的基准测试方法，并开源了实现代码以促进相关研究。
