---
title: "Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning"
authors:
  - "Antoine Bergerault"
  - "Volkan Cevher"
  - "Negar Mehr"
date: "2026-02-24"
arxiv_id: "2602.21020"
arxiv_url: "https://arxiv.org/abs/2602.21020"
pdf_url: "https://arxiv.org/pdf/2602.21020v1"
categories:
  - "cs.LG"
  - "cs.GT"
  - "cs.MA"
tags:
  - "多智能体系统"
  - "模仿学习"
  - "纳什均衡"
  - "离线学习"
  - "策略学习"
  - "博弈论"
  - "理论分析"
relevance_score: 7.5
---

# Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning

## 原始摘要

Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations of interactions in multi-agent interactive domains. Despite existing guarantees on the performance of the resulting learned policies, characterizations of how far the learned polices are from a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and demonstrating a new hardness result on characterizing the Nash gap given a fixed measure matching error. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium. Specifically, for the case of dominant strategy expert equilibria, assuming Behavioral Cloning error $ε_{\text{BC}}$, this provides a Nash imitation gap of $\mathcal{O}\left(nε_{\text{BC}}/(1-γ)^2\right)$ for a discount factor $γ$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决多智能体模仿学习（MA-IL）中的一个核心理论问题：如何从专家演示中学习到难以被利用的策略，即学习到的策略与纳什均衡之间的距离（可剥削性）如何被量化与保证。研究背景是，模仿学习在单智能体领域已取得显著成功，能够通过行为克隆或占用度量匹配等方式从专家示范中直接学习策略，避免了复杂奖励函数的设计。然而，在多智能体交互环境中（如合作或竞争场景），直接将单智能体方法扩展会面临根本性局限：学习到的策略可能容易被其他智能体通过单方面偏离策略而利用，即策略可能远离纳什均衡，导致性能不稳定或脆弱。

现有方法的不足在于，尽管已有工作关注多智能体模仿学习的性能保证，但对于离线设置下学习策略与纳什均衡之间的差距（即“纳什间隙”）缺乏严格的理论刻画。具体来说，现有方法通常隐含假设模仿误差为零时就能恢复纳什均衡，但论文指出这一一致性假设在一般马尔可夫博弈中并不成立。即使精确匹配专家的占用度量，也可能产生高度可剥削的策略，且计算紧密的可剥削性下界即使在已知游戏动力学时也是困难的。

本文要解决的核心问题是：在给定实际中可度量的模仿误差（如行为克隆误差或度量匹配误差）下，如何刻画学习策略的纳什间隙，并确定在何种条件下能够推导出一致（误差为零时间隙为零）且可高效计算的上界。为此，论文首先通过反例证明了一般n人马尔可夫博弈中学习低可剥削性策略的不可能性和硬度结果，揭示了现有方法的理论缺陷。然后，通过引入对专家均衡的战略优势假设（如占优策略均衡），以及提出一种新的“最佳响应连续性”概念，论文展示了如何在这些条件下构建一致且可处理的纳什间隙上界，从而为多智能体模仿学习提供了更可靠的理论基础。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为三类：单智能体模仿学习、多智能体模仿学习，以及多智能体模仿学习的理论障碍。

在**单智能体模仿学习（SA-IL）**方面，核心方法包括行为克隆（BC）、逆强化学习（IRL）和对抗式模仿学习。这些方法旨在从专家演示中拟合专家策略、奖励函数或占用度量，其性能通常用学习策略与专家之间的次优性差距来衡量。本文的研究建立在SA-IL的理论基础之上，但将其问题框架扩展到了多智能体交互领域。

在**多智能体模仿学习（MA-IL）**方面，已有工作将BC、对抗式模仿学习等方法从单智能体扩展到多智能体场景，并应用于自动驾驶、机器人交互等领域。然而，这些扩展方法通常缺乏对学习策略在战略交互中鲁棒性的理论保证。本文明确指出，现有方法（如BC和GAIL）学习到的策略在一般博弈中容易被利用，这正是本文要解决的核心问题。

关于**多智能体模仿学习的理论障碍**，先前研究已证明BC和GAIL策略在一般游戏中具有可剥削性。其中一些工作为多智能体行为克隆提供了首个遗憾上界，但其依赖的集中系数在一般游戏中难以处理且可能无界，导致界限不具一致性且难以实用。本文与这些工作的关系是：直接面对并深入分析了这些理论障碍（如精确度量匹配失败、纳什间隙表征的困难性），并进一步提出了克服这些挑战的条件（如利用专家均衡的战略优势假设、最佳响应连续性），从而推导出一致且可处理的纳什间隙界限，弥补了现有文献的空白。

### Q3: 论文如何解决这个问题？

论文通过引入“最佳响应连续性”这一新概念，并结合战略优势假设，为多智能体模仿学习中的可剥削性上界提供了理论保证。核心方法是首先证明在一般n玩家马尔可夫博弈中，仅凭行为克隆误差无法保证学习策略接近纳什均衡，甚至精确的度量匹配也可能失败。为此，作者转向分析专家均衡本身的性质。

整体框架基于对专家均衡施加结构性假设。当专家均衡是占优策略均衡时，论文推导出纳什模仿间隙的上界为O(nε_BC/(1-γ)^2)，其中ε_BC是行为克隆误差，γ是折扣因子。这一结果的关键在于，占优策略均衡具有内在的稳定性，使得其他智能体的策略微小偏离不会显著改变某个智能体的最优响应。

为了将这一结果推广到更一般的博弈类别，论文提出了“最佳响应连续性”的形式化定义。该定义量化了当对手策略π_{-i}以ε的误差偏离专家均衡策略π^E_{-i}时，智能体i的最优响应策略π_i*与专家策略π_i^E之间的最大偏差δ(ε)。如果一个博弈类别C是δ-连续的，那么对于该类中的所有博弈及其所有纳什均衡，都存在一个函数δ来描述这种连续性关系。

主要创新点在于利用这一连续性概念，将计算纳什间隙上界的复杂问题，转化为寻找一个有效的δ函数。通过分析δ的性质，可以推导出与博弈结构相关的、可处理的纳什间隙上界。论文进一步指出，即使对于具有一致价值间隙上界的博弈类别，也只能得到平凡的δ(ε)=2，这凸显了为获得非平凡上界而需要额外的结构性条件（如占优策略或连续性）的必要性。最终，该方法通过将策略的模仿误差与均衡的稳定性特性相结合，为离线多智能体模仿学习提供了可证明的、低可剥削性的策略学习理论框架。

### Q4: 论文做了哪些实验？

论文通过理论分析和构造性示例进行实验，而非传统的实证评估。实验设置聚焦于分析多智能体模仿学习（MA-IL）在马尔可夫博弈中学习低可剥削策略的理论可行性。主要实验包括：1）在精确占用度量匹配的理想情况下，证明即使满足完全状态支持，仅状态匹配仍可能导致线性纳什间隙（Nash gap），例如构造一个合作双玩家游戏，其中专家策略πE与学习策略π状态分布相同，但纳什间隙达到Ω(1/(1-γ))，关键指标为纳什间隙与有效视野1/(1-γ)成线性比例。2）证明在近似匹配误差下，计算紧纳什间隙下界是PPAD难的，即使已知博弈结构，该问题仍难以高效求解。3）提出最佳响应连续性（δ-continuity）概念，推导可处理的纳什间隙上界：对于具有主导策略专家均衡的情况，假设行为克隆误差ε_BC，纳什模仿间隙为O(nε_BC/(1-γ)^2)，其中γ为折扣因子，n为玩家数量。对比方法涉及传统占用度量匹配（状态匹配与状态-动作匹配），结果显示仅状态匹配不足以保证纳什均衡学习，而状态-动作匹配在完全状态支持下可恢复均衡。主要结果强调，MA-IL在一般博弈中学习低可剥削策略存在固有困难，但通过战略优势假设（如主导策略均衡）可提供一致性上界。

### Q5: 有什么可以进一步探索的点？

本文揭示了离线多智能体模仿学习（MA-IL）在一般马尔可夫博弈中学习低可利用性策略的根本困难，这为未来研究提供了几个关键方向。首先，论文的核心局限性在于其理论分析依赖于专家均衡满足特定策略优势（如占优策略）或最佳响应连续性假设，这些条件在复杂的现实交互中可能不成立。因此，一个重要的探索方向是放宽这些假设，研究在更一般的均衡类型（如混合策略纳什均衡或相关均衡）下，是否仍能推导出可计算的纳什间隙上界。

其次，论文提出的最佳响应连续性概念虽然有助于分析，但其具体形式（δ函数）的估计和验证在实践中可能非常困难。未来工作可以探索如何从有限的专家示范数据中自动学习或推断博弈的连续性结构，例如通过元学习或博弈结构识别技术。此外，将理论框架与具体的算法设计更紧密地结合是另一个有前景的方向。例如，可以开发新型正则化方法，在模仿学习过程中显式地鼓励策略满足连续性条件，从而直接优化纳什间隙的上界，而非仅仅最小化行为克隆误差。

最后，论文主要关注离线设置下的理论界限，未来可以扩展到在线或交互式模仿学习场景，其中智能体可以与环境或模拟对手进行交互以主动减少可利用性。同时，将分析从完全信息博弈扩展到部分可观测环境，并考虑更实用的评估指标（如对对抗性策略的鲁棒性），也是推动MA-IL走向实际应用的重要步骤。

### Q6: 总结一下论文的主要内容

该论文研究了多智能体模仿学习（MA-IL）中一个关键但被忽视的问题：从离线专家演示中学习到的策略距离纳什均衡有多远，即其可被利用的程度。论文首先揭示了在一般n人马尔可夫博弈中学习低可剥削性策略的固有困难，包括提供了精确度量匹配仍会失败的例子，并证明了在固定度量匹配误差下刻画纳什差距的新的计算困难性。核心贡献在于，论文指出通过假设专家均衡具有策略优势（如占优策略均衡），可以克服这些挑战。在专家遵循占优策略的假设下，论文证明了行为克隆误差ε_BC与所学策略的纳什模仿差距之间存在O(nε_BC/(1-γ)^2)的关系。作者进一步将此结果推广到一种新的“最佳响应连续性”概念，并论证标准的正则化技术隐式地鼓励了这种性质。该工作的意义在于首次系统分析了离线MA-IL的均衡保证局限性，并为在特定结构性假设下实现可证明的低可剥削性策略学习提供了理论框架。
