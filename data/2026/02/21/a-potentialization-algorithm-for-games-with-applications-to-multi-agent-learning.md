---
title: "A potentialization algorithm for games with applications to multi-agent learning in repeated games"
authors:
  - "Philipp Lakheshar"
  - "Sharwin Rezagholi"
date: "2026-02-21"
arxiv_id: "2602.18925"
arxiv_url: "https://arxiv.org/abs/2602.18925"
pdf_url: "https://arxiv.org/pdf/2602.18925v1"
categories:
  - "cs.MA"
  - "cs.GT"
tags:
  - "多智能体学习"
  - "博弈论"
  - "重复博弈"
  - "势博弈"
  - "算法"
  - "收敛性分析"
relevance_score: 6.5
---

# A potentialization algorithm for games with applications to multi-agent learning in repeated games

## 原始摘要

We investigate an algorithm that assigns to any game in normal form an approximating game that admits an ordinal potential function. Due to the properties of potential games, the algorithm equips every game with a surrogate reward structure that allows efficient multi-agent learning. Numerical simulations using the replicator dynamics show that 'potentialization' guarantees convergence to stable agent behavior.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决多智能体强化学习（MARL）中因环境非平稳性和智能体间相互影响导致的收敛困难问题。具体而言，当多个独立学习的智能体在共享环境中互动时，传统的单智能体算法（如独立Q学习）往往无法收敛，或会收敛到不理想的稳定状态（如低效的纳什均衡）。论文的核心思路是，为一般的标准形式博弈（normal-form games）设计一种“势化”（potentialization）算法，将其近似转化为具有序数势函数（ordinal potential）的博弈。势博弈具有良好的学习特性，许多近视学习算法能在其中保证收敛至纳什均衡。通过用势博弈的奖励结构替代原始游戏的奖励函数，并在尽可能保留原始激励结构的前提下引入最小的奖励扭曲，该算法为更广泛的MARL问题提供了一个可学习的代理奖励框架。这使得在重复博弈场景中，多智能体能够利用势博弈的收敛保障进行高效学习，从而克服传统MARL方法中的非平稳性和收敛失败问题。

### Q2: 有哪些相关研究？

本文的核心是提出一种“势化”（potentialization）算法，将任意标准形式博弈转化为近似博弈，使其具有序数势函数。相关研究主要围绕势博弈理论及其在多智能体学习中的应用展开。

首先，势博弈的经典定义和性质由 Monderer 和 Shapley 在 1996 年的工作中确立，他们证明了势博弈具有纯策略纳什均衡，且其偏差图无弱改进环。本文直接引用了这一基础理论。其次，Voorneveld 和 Norde 在 1997 年的研究给出了博弈存在序数势函数的充要条件（即偏差图无正权重的弱改进环），本文的算法设计在理论上以此作为关键判别准则。

在多智能体学习方面，Young 等人的工作（1993, 2004）表明，一大类近视的个体学习规则（如复制者动态）在势博弈中能收敛到纯策略纳什均衡。本文的“势化”算法正是为了将这一良好的收敛性保证“移植”到更广泛的非势博弈上。它通过构造一个具有相同偏差图拓扑但权重经过调整的近似博弈，为原博弈提供了一个替代的、可学习的奖励结构。因此，本文的工作与上述研究的关系是：它建立在势博弈的经典判别定理之上，并旨在通过算法手段，将 Young 等人证明的、在势博弈中有效的学习收敛性，扩展至一般博弈场景，从而为多智能体在重复博弈中的学习提供一种通用的稳定性保证框架。

### Q3: 论文如何解决这个问题？

该论文通过提出一种“势化”（potentialization）算法，将任意标准形式的博弈转化为一个近似博弈，该近似博弈具有序数势函数，从而为多智能体学习提供高效的收敛保证。核心方法分为三步：首先，基于非负偏差图计算调整图；其次，利用强连通分量概念，通过图凝聚技术构建序数势函数；最后，使用该势函数定义一个共同利益博弈（即势化博弈）。

算法具体流程如下：1）计算原始博弈的非负偏差图，其中顶点表示行动组合，边表示玩家单方面偏离的收益增益（权重为非负值）。2）计算该偏差图的凝聚图，即将强连通分量作为新顶点，边权重取原分量间边的最大权重。3）对凝聚图进行拓扑排序，从无入边的顶点开始，递归地为每个顶点分配势函数值：若无入边则设为零，否则取所有入边“起点势值+边权重”的最大值。4）将势函数值作为所有玩家的共同收益函数，得到势化博弈。

该算法的关键创新在于利用图论方法（强连通分量、拓扑排序）确保构造的势函数满足Voorneveld-Nolde条件（即强连通分量内边权重为零），从而保证势化博弈具有序数势函数性质。这使得多智能体在重复博弈中采用复制动力学等学习规则时，能收敛到稳定行为。算法复杂度为O(k^n)（k为最大行动数，n为玩家数），虽受玩家数量限制，但为任意博弈提供了可计算的势函数近似。此外，论文证明势化博弈保留原博弈的严格纳什均衡，且不会将严格占优行动转化为均衡，但可能引入新均衡或导致效用扭曲。

### Q4: 论文做了哪些实验？

论文通过数值模拟实验验证了“势化”算法在促进多智能体学习收敛性方面的有效性。实验设置如下：首先，随机生成两种规格的博弈：一种是2个玩家、每个玩家10个动作的(10×10)博弈，另一种是3个玩家、每个玩家4个动作的(4×4×4)博弈，各生成1000个实例。每个博弈的收益值从离散均匀分布中独立抽取，并归一化到[0,1]区间。实验过程包括：对原始博弈G应用势化算法得到势博弈G_Φ；分别对G_Φ和G模拟其复制者动态方程（使用四阶龙格-库塔方法，步长0.01），智能体的初始策略随机生成。收敛性通过策略变化量β（连续策略向量的最大L2范数差）连续1000步小于10^(-9)来判定；对于原始博弈，若在势化版本收敛所需的步数内未收敛，则停止模拟。

主要结果：在(10×10)博弈中，势化后的博弈有96.6%收敛，而原始博弈仅8.6%收敛；在(4×4×4)博弈中，势化后收敛率为90.6%，原始博弈为11.8%。势化博弈的策略变化曲线更平滑且始终低于原始博弈，显示出更高的稳定性。在平均期望收益方面，势化博弈略低但更平稳：(10×10)博弈中，原始博弈最终平均收益为0.79，势化博弈为0.76（相当于原始的96.4%）；(4×4×4)博弈中，两者分别为0.69和0.68（相当于原始的98.6%）。这表明势化算法能显著提升学习过程的收敛性，同时保持接近原始博弈的收益水平。

### Q5: 有什么可以进一步探索的点？

本文提出的“势化”算法将一般博弈转化为具有序数势函数的近似博弈，从而利用势博弈的良好性质促进多智能体学习。其局限性在于：1）势化过程可能导致平均奖励下降，即稳定性代价；2）目前仅适用于静态正规形式博弈，尚未扩展到随机或动态环境；3）算法对博弈结构的近似可能损失原博弈的竞争性信息。

未来可探索的方向包括：1）将算法扩展至随机博弈（多智能体马尔可夫决策过程），通过势化其隐含的瞬时延续博弈来实现；2）研究势化奖励结构与原始奖励的混合使用策略，例如在预热阶段使用势化奖励加速学习，再切换至原奖励以聚焦竞争部分；3）在强化学习框架中验证该方法的收敛性与效率，并探索其在复杂多智能体系统（如合作-竞争混合场景）中的实际应用潜力。

### Q6: 总结一下论文的主要内容

这篇论文提出了一种“势化”算法，能将任意标准形式的博弈近似为一个具有序数势函数的博弈。其核心贡献在于，通过构造一个替代的势化奖励结构，将原博弈转化为势博弈，从而利用势博弈的良好性质（如存在纯策略纳什均衡、学习动态易于收敛）来促进高效的多智能体学习。数值模拟使用复制者动态验证了该方法能保证智能体行为稳定收敛。

该算法的意义在于为多智能体强化学习提供了一种新的学习加速框架。它既可直接使用势化奖励进行学习，也可将其作为“预热”阶段工具，先快速收敛到博弈的“共同利益”部分，再切换回原始奖励以专注于“竞争”部分，从而平衡学习速度与最终收益。论文展望了将该方法扩展到随机博弈（多智能体马尔可夫决策过程）的未来方向。
