---
title: "DREAM: Deep Research Evaluation with Agentic Metrics"
authors:
  - "Elad Ben Avraham"
  - "Changhao Li"
  - "Ron Dorfman"
  - "Roy Ganz"
  - "Oren Nuriel"
  - "Amir Dudai"
  - "Aviad Aberdam"
  - "Noah Flynn"
  - "Elman Mansimov"
  - "Adi Kalyanpur"
  - "Ron Litman"
date: "2026-02-21"
arxiv_id: "2602.18940"
arxiv_url: "https://arxiv.org/abs/2602.18940"
pdf_url: "https://arxiv.org/pdf/2602.18940v1"
categories:
  - "cs.AI"
tags:
  - "Agent 评测/基准"
  - "Agent 架构"
  - "工具使用"
  - "研究智能体"
  - "评估框架"
  - "能力对等"
  - "无参考评估"
relevance_score: 9.0
---

# DREAM: Deep Research Evaluation with Agentic Metrics

## 原始摘要

Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决深度研究智能体（Deep Research Agents）生成的分析师级研究报告的评估难题。当前评估方法存在“合成幻象”问题，即报告表面流畅、引用得当，但可能隐藏着事实错误、时效性不足和逻辑缺陷。现有基准主要依赖静态评估（如人工标注或LLM作为评判者），缺乏对外部工具的调用能力，无法主动验证事实的时效性和正确性，导致评估维度不全面。

为此，论文提出了DREAM框架，其核心是引入“能力对等”原则，使评估过程本身具备智能体特性。DREAM通过一个结合静态指标和由工具调用智能体生成的动态指标的评估协议，实现了对报告关键信息覆盖度、推理质量和事实性的主动验证。该方法能够敏感地检测事实错误和时效性退化，从而提供一种无需参考标准、可扩展的评估范式。

### Q2: 有哪些相关研究？

本文的相关研究主要集中在深度研究评估（Deep Research Evaluation, DRE）的现有基准上。论文通过一个统一的四维分类法（呈现质量、任务合规性、分析深度、来源质量）系统梳理了当前的研究格局，并指出了不同评估范式的局限性。

相关工作主要包括以下几类：
1.  **人工定义评估标准**：例如 **ResearchRubrics**，它使用人工构建的详细评分标准来评估所有四个维度，虽然可靠但成本高昂（需2800多小时专家标注）。**DeepResearchGym** 则通过数据集特定的行为代理（如报告是否覆盖了用户历史上点击的文档）来编码任务合规性。**LiveResearchBench** 采用混合方法，结合了LLM生成的检查清单和人工验证。
2.  **闭环LLM评估**：为了提升可扩展性，这类研究用LLM生成评估标准。例如 **DeepResearch Bench** 使用RACE方法为多个维度生成加权评分标准，**DeepResearch Arena** 则使用ACE方法移除引用并生成查询特定的检查清单。然而，这些方法由静态LLM执行，缺乏外部工具访问、时间上下文或独立证据收集能力，限制了其对基于事实的推理和动态任务要求的评估。
3.  **引用对齐工作流**：这类方法在“来源质量”维度中占主导，通过多步骤流程验证报告中的声明与其引用的URL内容是否一致。**DeepResearch Bench (FACT)**、**DeepResearchGym (Retrieval Faithfulness)** 和 **LiveResearchBench (Citation Accuracy)** 都采用了此类流程。但它们主要衡量的是*内在的*引用忠实度，而非*外在的*事实正确性，且验证范围局限于提供的引用，因此无法检测基于过时或不可靠来源的声明，导致了“引用对齐谬误”。

本文与这些研究的关系是批判性的继承与发展。它指出，现有基准普遍存在“系统性失衡”和“评估者能力不匹配”问题：它们过度评估了表面流畅性和任务合规性，却严重缺乏对事实正确性、时间有效性和逻辑推理的直接评估。这导致了“综合海市蜃楼”现象。本文提出的DREAM框架正是为了从根本上解决这一能力不匹配问题，其核心创新在于将评估本身也“智能体化”，让评估者具备与被评估研究智能体同等的工具使用（如网络搜索）和能力，从而能够进行时间感知、基于证据的验证和系统性推理探测。

### Q3: 论文如何解决这个问题？

DREAM框架通过构建一个**智能体化的评估流程**来解决深度研究报告评估中的“能力不匹配”问题。其核心方法是**能力对等原则**，即让评估者自身具备与被评估的研究智能体相当甚至更强的工具使用和深度推理能力，从而能够有效检测报告中的事实错误、时效性问题和推理缺陷。

**架构设计**分为两个阶段：
1.  **协议创建阶段**：针对特定研究问题，动态构建一个包含**静态指标**和**自适应指标**的评估协议。静态指标（如写作质量、事实性、引用完整性、领域权威性）是任务无关的通用标准。自适应指标则由一个**协议创建智能体**生成，该智能体利用检索工具（如网络搜索、ArXiv）进行轻量级研究，产出两项关键内容：**关键信息覆盖度**（将核心事实转化为可验证的是/否问题清单）和**推理质量**（生成需结合外部证据进行交叉验证的结构化验证计划）。这确保了评估标准本身是深入、具体且与时俱进的。

2.  **协议执行阶段**：根据指标需求，将评估任务路由给具备相应能力的最简评估器执行，形成三层评估架构：
    *   **LLM评估器**：处理无需外部工具的评判任务，如基于固定量规评估写作质量，或根据智能体生成的清单核对关键信息覆盖度。
    *   **智能体评估器**：由一个代码智能体担任，专门执行**推理质量**评估。它自主遵循第一阶段创建的验证计划，调用工具检索外部证据，并基于证据支持程度对报告的推理连贯性和有效性进行评分。
    *   **工作流评估器**：实施三个自动化的验证流水线。对于**引用完整性**，计算引用覆盖率和引用忠实度的调和平均；对于**事实性**，通过生成中性化搜索查询独立于引用来验证声明；对于**领域权威性**，则评估引用来源域名的可信度。

**关键技术**包括：1) 利用工具调用智能体进行动态、时效性感知的协议生成；2) 通过中性化查询进行独立的事实核查；3) 将引用验证分解为内部一致性和外部真实性两个维度；4) 采用基于能力对等的评估器路由机制。这套组合拳使得DREAM能够系统性地探测表面流畅性之下的深层次缺陷，实现了无需固定参考答案的、可扩展的深度研究评估。

### Q4: 论文做了哪些实验？

论文在三个数据集上对领先的深度研究智能体（DRA）进行了基准测试：DeepResearch Bench（英文子集）、LiveResearchBench（80个公开查询）和ResearchRubrics（101个查询）。评估对象为三个开源系统：基于GPT-5的LangChain Open Deep Research、基于Claude Opus 4.6的Smolagents Open Deep Research以及Tongyi Deep Research。

主要结果揭示了几个关键模式：所有智能体的“引用完整性”得分都极低，表明存在系统性引用缺陷。具体而言，Smolagents和Tongyi的引用完整性接近零，主要因为很少将主张关联到具体来源；LangChain的引用完整性稍高，但引用的忠实性很低。在内容质量方面，Smolagents在事实性、写作质量、关键信息覆盖和推理质量等多项指标上领先，尽管其引用规范几乎缺失。Tongyi在事实性上排名第二，但在自适应推理指标上落后；LangChain的事实性得分最低，但推理质量优于Tongyi。

此外，论文还进行了鲁棒性分析，使用DeepSeek-V3.2和Kimi-K2.5等不同大模型作为评估协议的执行骨干。结果表明，尽管绝对分数因模型校准存在微小波动，但各智能体的相对性能排名高度一致，证明了所提评估指标的稳定性。

### Q5: 有什么可以进一步探索的点？

该论文提出的DREAM框架虽在评估深度研究智能体方面有显著改进，但仍存在几个关键局限与未来探索方向。局限性主要包括：一是依赖外部工具可能导致服务可用性问题与检索偏差，牺牲了封闭一致性以换取时效性；二是智能体化评估计算成本高，多步验证与工具交互增加了延迟与开销；三是仅评估最终输出，未涵盖研究过程（如搜索轨迹效率）。未来方向可着重于：优化计算效率，例如通过缓存或选择性评估策略降低开销；扩展评估维度，纳入过程级遥测数据以全面分析研究动态；探索更稳健的工具集成方法，减少第三方依赖风险；以及将评估框架适配至更广泛的研究任务领域，提升其通用性与可扩展性。

### Q6: 总结一下论文的主要内容

这篇论文针对深度研究智能体生成高质量报告后的评估难题，提出了一个创新的评估框架DREAM。核心贡献在于揭示了现有评估方法存在“合成幻象”问题，即仅依赖表面流畅度和引用对齐的静态评估器（如LLM-as-a-judge或基于参考的基准）无法有效检测事实准确性、时效性和逻辑推理等深层缺陷，这源于评估器能力与被评估智能体能力之间的不匹配。

为解决这一问题，论文提出了“能力对等”原则，并实例化为DREAM框架。该框架的核心是使评估过程本身具备智能体特性，通过结合查询无关的指标与由工具调用智能体生成的动态自适应指标，构建了一个动态的、配备工具的评估协议。这使得评估能够进行时效性感知的事实核查、基于证据的验证以及系统性的推理探测。

论文的意义在于为下一代前沿深度研究智能体的评估提供了一个可扩展、无需参考标准的蓝图。实验表明，DREAM在检测事实错误和时效性退化方面比现有基准显著更敏感，标志着评估范式必须随着智能体能力的进化而同步发展，从依赖表面形式转向基于证据和自主行动的实质性评估。
