---
title: "Personalized Help for Optimizing Low-Skilled Users' Strategy"
authors:
  - "Feng Gu"
  - "Wichayaporn Wongkamjan"
  - "Jonathan K. Kummerfeld"
  - "Denis Peskoff"
  - "Jonathan May"
  - "Jordan Boyd-Graber"
date: "2024-11-14"
arxiv_id: "2411.09109"
arxiv_url: "https://arxiv.org/abs/2411.09109"
pdf_url: "https://arxiv.org/pdf/2411.09109v4"
categories:
  - "cs.CL"
tags:
  - "Agent 评测/基准"
  - "人机交互"
  - "个性化 Agent"
  - "游戏 AI"
  - "自然语言 Agent"
  - "策略建议"
relevance_score: 7.5
---

# Personalized Help for Optimizing Low-Skilled Users' Strategy

## 原始摘要

AIs can beat humans in game environments; however, how helpful those agents are to human remains understudied. We augment CICERO, a natural language agent that demonstrates superhuman performance in Diplomacy, to generate both move and message advice based on player intentions. A dozen Diplomacy games with novice and experienced players, with varying advice settings, show that some of the generated advice is beneficial. It helps novices compete with experienced players and in some instances even surpass them. The mere presence of advice can be advantageous, even if players do not follow it.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在探索AI在复杂策略游戏中如何为人类玩家提供个性化帮助，特别是针对低技能（新手）用户。研究背景是，尽管AI（如CICERO）在《外交》等游戏中已展现出超越人类的决策能力，但AI如何有效辅助人类玩家、提升其策略水平仍缺乏深入探讨。现有方法多集中于开发高性能的游戏AI代理，但这些代理通常作为独立玩家参与，而非以“顾问”角色提供实时建议，忽略了人类与AI在技能上的互补性（例如人类更擅长沟通，AI更擅长策略计算）。现有研究的不足在于，未能系统评估AI生成建议（包括行动和消息建议）对玩家实际表现的影响，尤其是对不同技能水平玩家的差异化帮助效果。

本文的核心问题是：如何设计一个AI顾问系统（名为Chiron），基于玩家意图和游戏状态提供个性化的行动与消息建议，从而增强人类与AI的协作，帮助低技能玩家在复杂策略游戏中快速提升水平，甚至与经验丰富的玩家竞争。论文通过实验验证了此类建议的有效性，并探讨了AI如何优化人类学习体验。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为三类：AI与人类协作中的适当依赖、作为玩家伙伴的AI智能体，以及增强学习应用。

在**AI适当依赖**方面，现有研究关注人类在AI辅助决策中的依赖程度，探讨如何减少过度依赖，并检验解释如何影响人类对AI的信任。然而，不同领域的实证结果存在矛盾：有些表明AI解释能改善人类决策，另一些则发现即使解释错误，人类仍会过度依赖。本文发现，在《外交》游戏中，玩家对AI建议相对保守，新手也不会盲目跟随。

在**AI作为玩家伙伴**方面，已有许多AI智能体在游戏中展现超人类表现，如Deep Blue、AlphaGo、AlphaStar等，但这些研究主要关注游戏结果而非如何塑造人类游戏行为。部分研究开发了自适应AI伙伴，能根据玩家经验调整行为，或评估人类对AI建议的依赖程度，但它们未在生成指导时考虑玩家意图。相比之下，本文的AI基于玩家过去的消息和行动生成个性化建议。

在**增强学习应用**方面，这是一种通过个性化增强学习体验的教育方法。传统上通过同伴互动模拟社交以辅助学习，近期研究利用AI和NLP智能体实现自适应教学交互，如在《龙与地下城》中应用心理理论生成指导，或开发叙事辅导系统帮助儿童学习。本文将增强学习概念应用于帮助新手理解《外交》游戏。

### Q3: 论文如何解决这个问题？

论文通过构建一个名为 \name{} 的辅助系统来解决低技能用户在复杂游戏环境中策略优化的问题。该系统基于在《外交》游戏中表现出超人类水平的自然语言智能体 CICERO，对其进行增强，使其能够根据玩家的意图生成行动建议和消息建议，从而为玩家提供个性化帮助。

核心方法是将 CICERO 从一个主动的游戏参与者，改造为一个被动的观察与建议生成器。整体框架是：\name{} 被动观察游戏状态（包括棋盘局势和玩家的消息历史），当玩家发送消息或每回合行动时，系统会基于最新的上下文重新计算建议，并呈现给用户。实验设计了四种随机分配的建议设置：无建议、仅消息建议、仅行动建议、以及两者兼备的建议，以对比不同干预方式的效果。

主要模块/组件包括：
1.  **建议生成模块**：基于 CICERO 的底层能力，根据当前游戏状态和玩家意图，生成两类输出：(a) 行动建议，即推荐一组具体的单位移动指令；(b) 消息建议，即建议玩家向谁发送消息以及消息内容。
2.  **实验与评估框架**：构建了一个包含新手和经验玩家的多轮游戏测试环境。通过线性回归模型量化建议的有效性，模型特征包括玩家角色、游戏回合数、玩家类型（新手/老手）和建议设置。同时，设计了“接受率”、“一致性”和“等价性”等多个指标来评估玩家对建议的依赖程度。

关键技术及创新点在于：
*   **意图驱动的个性化建议生成**：系统不是提供通用策略，而是基于对玩家当前意图的理解来生成建议，使帮助更具针对性。
*   **被动观察与主动建议的结合**：系统不直接参与游戏，避免了代理完全接管玩家决策的问题，保持了玩家的主体性，同时又能提供实时、情境化的辅助。
*   **对“建议存在效应”的验证**：研究发现，即使玩家不遵循具体的建议内容，仅仅是建议的存在本身也能对玩家（尤其是新手）产生积极影响，帮助他们更好地与经验玩家竞争甚至超越。这揭示了AI辅助的一种潜在心理或认知机制。
*   **细粒度的评估指标**：除了简单的接受率，还引入了“一致性”来衡量玩家行动与建议之间的部分重叠，这能更精准地捕捉玩家如何有选择性地采纳建议，而非全盘接受或拒绝。

### Q4: 论文做了哪些实验？

实验设置方面，研究在经典七人棋盘游戏《外交》中进行，通过修改游戏引擎和界面，让名为Pholus的AI助手（基于CICERO构建）被动观察游戏并为玩家提供建议。每局游戏包含2至5名人类玩家，持续约三小时，每回合十分钟。玩家被随机分配至四种建议设置之一：无建议、仅消息建议、仅行动建议、消息与行动建议结合。Pholus会根据游戏状态和消息历史动态生成建议。

数据集与参与者方面，研究共收集了12局游戏的数据，涉及41名玩家，包括新手（无游戏经验）和经验丰富的老手。实验记录了超过3600条行动建议和4300条消息建议的交互数据。

对比方法主要涉及不同建议设置下的玩家表现对比，以及新手与老手在建议接受度和游戏结果上的差异。关键评估指标包括：建议接受率、每回合得分净增益（通过带正则化的线性回归模型分析）、行动一致率（Agreement）和等价率（Equivalence），以及消息内容的相似度（使用AMR解析和SMATCH分数衡量）。

主要结果显示：首先，建议整体有益，无建议设置与得分略微下降相关（回归系数约-0.05），同时接收行动建议与得分增益呈正相关，而同时接收两种建议的正面影响最大。其次，新手更愿意接受建议，其行动建议接受率达32.6%（老手仅6.4%），消息建议接受率6.3%（老手3.4%）。在建议帮助下，新手能与老手表现相当甚至击败他们，在接收双建议的五局游戏中，仅一名新手提前出局。此外，消息分析表明玩家常对建议进行小幅修改（SMATCH分数较高），但也会因策略目标不同而完全忽略建议（SMATCH分数为0）。这些结果验证了个性化建议对低技能用户的策略优化具有积极帮助。

### Q5: 有什么可以进一步探索的点？

该研究存在几个值得深入探索的方向。首先，在技术层面，当前系统生成的建议有时过于通用或与玩家意图不符，这源于模型训练数据更侧重于个体最优解而非合作共赢。未来可探索如何让AI更好地理解并支持玩家的高层次战略意图（如结盟、长期目标），而不仅仅是基于即时效用生成建议。这可能需要引入更复杂的意图识别模块和基于元级策略的个性化建议生成。

其次，在可访问性与实用性方面，当前模型依赖高端GPU且内存占用大，限制了普及。未来研究可专注于模型轻量化，例如通过知识蒸馏或模型压缩技术开发更高效的版本，使其能在普通设备上运行，从而惠及更广泛的玩家和研究者。

此外，论文发现即使玩家不遵循建议，其存在本身也能带来优势，这暗示了AI建议可能具有心理或认知层面的辅助作用。未来可进一步研究这种“建议在场效应”的心理机制，以及如何设计建议的呈现方式以最大化这种辅助效果，例如通过更自然的交互界面或动态调整建议的详细程度。

### Q6: 总结一下论文的主要内容

该论文探讨了AI在游戏环境中如何为人类玩家提供个性化帮助，特别是针对低技能用户。核心问题是研究AI生成的建议（包括行动和消息）是否能有效提升玩家表现，尤其是新手玩家。论文以《外交》游戏为实验平台，扩展了CICERO智能体，使其能根据玩家意图生成实时建议。

方法上，研究设计了多组实验，让新手和经验玩家在不同建议设置下进行游戏，对比分析建议的接受度和影响。主要结论表明，即使玩家不完全遵循建议，AI建议的存在本身就能显著改善新手玩家的决策和游戏结果，甚至帮助他们与经验玩家竞争或超越。这揭示了AI建议的潜在价值不仅在于直接执行，更在于启发和优化玩家的战略思考。

论文的贡献在于实证展示了个性化AI建议在人类-AI协作中的积极作用，为未来研究如何让AI在不替代人类决策的前提下提供有效指导提供了新视角。
