---
title: "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking"
authors:
  - "Jingcheng Yang"
  - "Tianhu Xiong"
  - "Shengyi Qian"
  - "Klara Nahrstedt"
  - "Mingyuan Wu"
date: "2026-02-23"
arxiv_id: "2602.20330"
arxiv_url: "https://arxiv.org/abs/2602.20330"
pdf_url: "https://arxiv.org/pdf/2602.20330v1"
categories:
  - "cs.CV"
  - "cs.AI"
  - "cs.LG"
tags:
  - "模型可解释性"
  - "多模态模型"
  - "视觉语言模型"
  - "内部机制分析"
  - "因果分析"
  - "模型调试"
relevance_score: 5.5
---

# Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking

## 原始摘要

Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决视觉-语言模型（VLMs）内部工作机制不透明的问题。尽管VLMs在视觉问答、图像描述等任务上表现出色，但其决策过程如同“黑箱”，缺乏可解释性。这在高风险领域（如医疗影像、自动驾驶）中尤为关键，因为难以诊断错误、减少偏见或确保模型与人类价值观对齐。

现有方法主要集中在纯文本模型的解释性研究上，例如通过注意力可视化、探测或电路发现来分析语言模型。然而，这些方法无法直接适用于VLMs，因为VLMs需要整合视觉和语言两种不同统计特性与语义的模态，并建立跨模态关联。目前，对于VLMs如何绑定视觉特征与文本标记、实现跨模态推理或协调视觉与语言注意力等内部机制，仍缺乏系统性的分析工具。

本文的核心问题是：如何揭示VLMs内部的多模态推理机制？为此，论文提出了首个针对VLMs的电路追踪框架，通过引入转码器、归因图和基于注意力的方法，系统性地分析模型内部的计算图。该框架不仅能追溯从视觉输入到最终输出的信息流，还能验证电路中的因果性与可控性，从而为构建更可解释、可靠的VLMs奠定基础。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为三类：机制可解释性、稀疏自编码器与转码器，以及视觉-语言模型可解释性。

在**机制可解释性**方面，已有研究致力于对语言模型进行因果逆向工程，超越早期的相关性分析方法。相关工作包括对特定行为（如归纳头、IOI任务、事实回忆/定位）的电路级分析、基于干预的定位方法（如激活修补和路径修补），以及基于叠加原理的特征级方法（如使用稀疏自编码器获取比单个神经元更可解释的特征）。本文借鉴了这些因果分析思想，但将其首次系统性地应用于视觉-语言模型。

在**稀疏自编码器与转码器**方面，稀疏自编码器通过学习稀疏、过完备的分解来获得更可解释的单义特征，缓解了神经表征的多义性问题。转码器通过用SAE替换MLP层来暴露特征级结构，已在语言模型分析中得到应用。本文的创新点在于首次将转码器应用于VLMs，以实现端到端的归因和电路追踪。

在**视觉-语言模型可解释性**方面，现有工作多集中于高层分析，如注意力可视化、对学习到的视觉概念进行探测，以及分析跨模态对齐。近期研究尝试定位VLMs中的视觉知识或理解视觉信息如何处理，但通常孤立分析单个组件或依赖相关性分析。本文与这些工作的核心区别在于，首次将**电路追踪**方法应用于VLMs，系统性地识别多模态推理背后的计算机制，强调因果性而非相关性。

### Q3: 论文如何解决这个问题？

论文通过提出一个名为“透明电路追踪”的框架来解决视觉语言模型（VLM）内部机制不透明的问题。该框架的核心目标是系统地分析和解释VLM在进行多模态推理时，如何层次化地整合视觉与语义概念。其整体方法可以概括为几个关键步骤和组件。

首先，框架采用**转码器**来替代模型中的多层感知机（MLP）子层。转码器本质上是一种稀疏自编码器，它将MLP的输入映射到一个高维、稀疏的潜在特征空间，然后再解码回原始输出维度。与传统的稀疏自编码器（SAE）不同，转码器直接模仿MLP的输入-输出行为，保持了计算等价性，同时将原本多义性的MLP表示分解为稀疏、近似单义性的特征，使其更易于解释。训练时，通过保留前k个最大激活值来直接强制稀疏性，而非依赖L1惩罚，这使得训练更稳定。

其次，为了揭示特征之间的因果关系，框架构建了**归因图**。在给定特定输入提示的情况下，由于转码器替换了MLP，且所有非线性操作（如ReLU、注意力）的值被冻结，模型在该输入点附近变得局部线性。归因图以节点（代表词元嵌入、活跃的转码器特征或输出logits）和边（代表特征间的贡献权重）的形式，线性地分解了底层特征如何影响上层激活并最终影响输出logits。通过修剪贡献微小的边，得到一个稀疏、可解释的计算图，清晰地展示了信息流动路径。

接着，为了理解每个稀疏特征的含义，框架进行了**特征激活分析**。通过在一个多样化的视觉-语言数据集上运行模型，收集最能激活每个特征的输入示例（图像-文本对），并分析其共同模式，从而人工标注特征所代表的概念（如“数学符号”、“特定物体”）。

此外，为了可视化视觉编码器关注了图像的哪些区域，框架计算了**注意力 rollout 图**。该方法通过聚合视觉编码器最后几层中注意力最集中的头部，生成热力图，将抽象的视觉特征激活与具体的图像空间位置关联起来，辅助专家理解多模态关联的机制。

最后，为了因果验证所发现的电路，框架采用了**干预与导向**方法。这包括特征导向（通过直接修改特定特征的激活值来增强或抑制其影响）和电路修补（将从一个电路中提取的激活模式移植到另一个电路中）。通过观察模型输出行为的变化，可以验证所识别的特征子图（即“电路”）是否确实是产生特定能力（如数学推理）的因果性计算单元。

**创新点**在于：1）首次将转码器与归因图分析系统性地结合，应用于VLM的电路追踪；2）提出了一个从特征分解、因果图构建到人工解释与因果验证的完整分析流程；3）通过特征导向和电路修补实验，不仅揭示了VLM内部存在处理不同任务（如数学推理）的特定视觉特征电路，还证明了这些电路是因果可控的，为构建更可解释、可靠的VLM奠定了基础。

### Q4: 论文做了哪些实验？

论文实验围绕视觉语言模型（Gemma-3-4B-it）的电路追踪框架展开。实验设置包括：使用SigLIP视觉编码器处理图像，生成4096个初始图像块标记，并池化为256个软图像标记，与文本标记拼接后输入34层Transformer解码器。数据集方面，训练转码器使用了多模态混合数据集：纯文本数据集SmoLIM2-135M-10B（144,000个样本）、ImageNet（144,000个图像-标题对）以及视觉推理问答数据集Cauldron（72,000个样本）。此外，特征分析和电路发现使用了ImageNet（18,000图像）和Cauldron（10,000图像）的子集，以及专门构建的小型任务相关图像集（20-100张图像）。

对比方法主要涉及不同训练配置下的转码器性能评估。关键实验包括：1）扩展因子（N_latents）对死潜在单元比例的影响，当N_latents为32、64、128时，早期层（如第3层）死潜在单元比例较高，而中间层（如第15层）激活更密集；2）训练数据模态对特征可解释性的影响，多模态数据训练的转码器在所有层均获得更低的未解释方差分数（FVU），中间层差距最大（表明视觉信息在此集成），高层差距减小；3）电路验证通过特征引导和激活修补进行，例如在火星相关电路中抑制火星视觉特征并激活地球视觉特征后，模型输出成功转向地球相关概念。

主要结果证实：所发现的视觉特征电路具有因果性和可控性，能够处理数学推理并支持跨模态关联。关键指标包括：转码器训练使用8块H100 GPU耗时约60小时；单个问答任务的属性图计算需20分钟；特征激活计算（约1000个特征）需20 GPU小时；注意力图预计算（28,000张图像）耗时2小时，生成约2TB数据。这些实验为理解VLM的多模态推理内部机制奠定了基础。

### Q5: 有什么可以进一步探索的点？

该论文的局限性及未来研究方向可从多个维度深入。首先，视觉特征解释的精确度不足，现有注意力图难以准确定位关键区域或区分计算中介特征与输出特征，未来可探索更细粒度的视觉归因方法，例如结合空间语义分割或动态注意力聚焦机制。其次，跨层特征叠加问题突出，当前逐层解码器设计无法捕捉跨层特征交互，导致大量近似重复特征出现，需研发自适应跨层解码架构（如引入稀疏编码的层次聚类）以提升特征解耦效率。再者，计算成本高昂限制了方法扩展性，需开发自动化特征解释与电路发现工具，例如利用轻量级元网络预测关键特征子集，或设计基于强化学习的电路搜索算法。此外，当前研究仅针对单一模型（Gemma3），其双向注意力等特殊机制可能影响结论普适性，未来应扩展至多模态架构（如DETR式编码器或扩散模型），验证电路机制的跨模型一致性。最后，人工依赖性强阻碍量化评估与应用落地，可通过引入语义嵌入聚类自动标注特征、构建简化因果图抽象表示，或将电路发现与模型微调结合（如基于识别电路进行对抗性增强），推动可解释性研究向实际优化闭环演进。

### Q6: 总结一下论文的主要内容

该论文提出了首个针对视觉-语言模型（VLMs）的电路追踪框架，旨在揭示其多模态推理的内部机制。核心贡献在于通过系统化方法将VLMs从“黑箱”转变为可解释的系统，从而理解视觉与语义概念如何分层整合。方法上，利用转码器提取可解释特征，结合归因图追踪因果结构，并采用基于注意力的方法，成功分离出处理物体识别、计数、问答和描述等任务的稀疏电路。实验通过特征引导和电路修补验证了这些电路的因果性和可控性。主要结论表明，VLMs中存在处理数学推理和支持跨模态关联的独立视觉特征电路，且这些电路可被精准干预。该框架不仅增进了对多模态思维的科学理解，还为调试模型、减少故障及设计更可解释的VLMs提供了实用工具，推动了透明、可控且对齐的AI系统发展。
