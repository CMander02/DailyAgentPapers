---
title: "Classroom Final Exam: An Instructor-Tested Reasoning Benchmark"
authors:
  - "Chongyang Gao"
  - "Diji Yang"
  - "Shuyan Zhou"
  - "Xichen Yan"
  - "Luchuan Song"
  - "Shuo Li"
  - "Kezhen Chen"
date: "2026-02-23"
arxiv_id: "2602.19517"
arxiv_url: "https://arxiv.org/abs/2602.19517"
pdf_url: "https://arxiv.org/pdf/2602.19517v1"
categories:
  - "cs.AI"
  - "cs.CE"
  - "cs.CL"
  - "cs.CV"
tags:
  - "Agent Benchmark"
  - "Reasoning Evaluation"
  - "Multimodal Benchmark"
  - "STEM"
  - "Diagnostic Analysis"
  - "Step Efficiency"
  - "Error Accumulation"
relevance_score: 7.5
---

# Classroom Final Exam: An Instructor-Tested Reasoning Benchmark

## 原始摘要

We introduce \CFE{} (\textbf{C}lassroom \textbf{F}inal \textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决当前大语言模型和基础模型评估中存在的两个核心问题：基准测试的饱和性与现实性不足，以及模型在需要深度领域知识和多步推理的复杂科学、技术、工程和数学问题上表现不佳。

研究背景是，尽管大语言模型在众多基准测试上取得了快速进展，但许多广泛使用的测试集已逐渐趋于饱和，难以有效区分前沿模型的真实能力。同时，现有研究表明，即使是最先进的模型，在面对需要深厚专业知识和多步骤逻辑推导的STEM领域问题时，仍然存在显著困难。

现有方法的不足主要体现在两个方面：一是许多基准测试缺乏真实性和区分度，可能包含大量简单的是非题或选择题，无法充分挑战模型的高级推理能力；二是传统的评估方法（如直接比较长文本输出与参考答案）容易产生误判，无法精确衡量模型对核心答案的掌握情况。

因此，本文的核心目标是构建一个更可靠、更具鉴别力且贴近真实学术场景的评估基准。具体而言，论文引入了“课堂期末考试”基准，它通过从大学真实课程作业和考试题目中精心筛选，形成了一个覆盖20多个STEM学科、包含文本和多模态问题的数据集。该基准不仅问题质量高、经过教学实践反复检验，还设计了一套基于变量的严格验证协议来精确评估模型输出，从而更准确地衡量模型在真实学术标准下的领域知识基础和推理能力。此外，论文还通过诊断性分析，旨在深入理解模型在多步推理中失败的具体原因，例如维持正确中间状态的能力不足和推理步骤效率低下等问题。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为两大类：**推理能力评估类**和**推理基准构建方法类**。

在**推理能力评估**方面，已有研究揭示了模型能力的差异。例如，MATH、AIME等基准评估竞赛风格的数学能力；OmniDocBench、CharXiv等则针对特定模态（如文档解析、图表解读）。这些工作表明，前沿模型在解决孤立、高复杂度问题或检索领域事实方面表现出色。然而，本文指出，这些在专项基准上的优异表现，并不等同于对需要整合广泛领域知识与多步逻辑推导的大学标准课程的系统性掌握。CFE-Bench正是为了填补这一评估空白而提出。

在**推理基准构建方法**上，现有工作主要分为两种路径。第一种是依赖新标注或合成数据，以确保新颖性和难度，如SimpleQA、FACTS和HLE。但这些基准可能因未经长期教学检验而存在标注错误或模糊性，且往往侧重于短答案准确性或任务复杂性，而非对长篇幅、组合式推理过程的验证。第二种是利用真实、久经考验的教学材料，如ScienceQA、MMLU系列和MMMU系列。它们确保了教学相关性，但主要采用基于结果的度量（如选择题准确率），且前沿模型在这些数据集上的性能正迅速接近饱和。此外，它们通常缺乏详细的、可作为中间检查点的分步推理解释。

**本文工作（CFE-Bench）与上述研究的区别和关系在于**：它继承了第二种路径，使用真实、反复使用的大学作业和考试题目，确保了材料的教学有效性和时间检验性。同时，它超越了传统的结果导向评估，通过引入由课程教师提供的、可分解为推理流的参考解决方案，建立了一个分步评估框架。这使得CFE-Bench不仅能评估最终答案，还能对模型的中间推理状态进行诊断性分析，从而更深入地评估其系统性推理能力，这是现有基准普遍欠缺的。

### Q3: 论文如何解决这个问题？

论文通过构建一个名为CFE的多模态基准测试，并引入一套结构化的评估框架来解决现有大语言模型在复杂多步推理任务中表现不佳的问题。其核心方法、架构设计和关键技术如下：

**整体框架与主要模块：**
1.  **基准构建与数据收集**：从真实的大学课程资源（如考试、作业）中收集问题，确保问题具有真实的难度和教学验证背景。收集后，通过LLM辅助检查和专家评审进行数据清洗和标准化，最终形成包含305个纯文本问题和144个多模态问题的基准数据集。
2.  **专家评审与标注**：招募领域专家对每个问题进行严格审查，过滤过于简单或模糊的题目，确保答案明确且可验证。对于多模态样本，额外评估其对视觉信息的依赖性。此过程通过一个专用的标注界面进行标准化，以减少标注方差。
3.  **结构化评估框架（核心创新）**：为了克服传统端到端评估（Long-to-Long, L2L）的缺陷（如部分正确幻觉、上下文诱导错误），论文提出了基于变量提取的结构化验证方法，称为“短对短”（Short-to-Short, S2S）验证。
    *   **变量标注**：为每个问题，由专家标注一组真实变量元组，包括变量名、语义描述、目标值和类型（如数值、公式）。
    *   **评估流程**：给定模型生成的回答，首先使用一个评判模型（如GPT-mini）根据变量描述提取出模型预测的对应值，然后将每个提取值与真实值进行比对，判定每个变量的正确性。只有当所有变量都正确时，整个问题才被判为正确。
4.  **评估协议对比**：除了S2S，论文还设置了另外两种评估设置作为对比：**长对短（L2S）**（将模型长回答与标注的变量值比对）和传统的**长对长（L2L）**（将模型长回答与完整参考答案直接比对）。实验表明，S2S与专家标注的一致性最高，且显著减少了误报。

**创新点：**
1.  **真实性与高质量数据源**：CFE基准完全基于经过课堂反复测试的真实教学材料构建，并由领域专家严格审核，确保了问题的现实难度和高质量。
2.  **细粒度、抗干扰的评估协议**：提出的S2S结构化评估框架是核心创新。它将复杂的推理答案分解为具体的、类型化的变量，使评估锚定在可验证的中间状态上，有效避免了传统L2L评估中因文本流畅性或部分正确性导致的误判，提供了更可靠、更细粒度的能力度量。
3.  **互补的评估指标**：引入了**变量准确率**（衡量每个问题中正确预测变量的平均比例）和**问题准确率**（要求所有变量完全正确）两个互补指标，能够更细致地反映模型在推理步骤中的部分进展和整体能力。
4.  **诊断性分析**：基于对参考答案推理流的分解，论文不仅能给出总体分数，还能诊断模型失败的具体模式（如难以维持正确的中间状态、推理步骤效率低下），为后续模型改进提供了明确方向。

总之，论文通过构建一个高质量、高难度的真实世界推理基准，并配套开发一套精准、结构化的自动评估框架，系统地揭示和评估了大语言模型在复杂多步STEM推理中的核心缺陷，为解决该问题提供了可靠的评估工具和深刻的诊断见解。

### Q4: 论文做了哪些实验？

论文实验主要包括模型性能评估和诊断分析两部分。

**实验设置与数据集**：实验在CFE-Bench基准上进行，该基准包含超过20个STEM领域的449个问题（305个纯文本子集和144个多模态子集），源自真实的大学作业和考试题，并由课程教师提供参考答案。评估采用思维链提示策略，解码温度通常设为0.7，对于推理模型限制思考生成为16,000个token。

**对比方法与主要结果**：评估了包括Gemini、GPT、Claude、Qwen、Llama等系列在内的开源和专有模型。关键指标为变量准确率（Variable Accuracy）和问题准确率（Question Accuracy）。在整体（文本+多模态）上，表现最佳的Gemini-3.1-pro-preview问题准确率为59.69%，第二名Gemini-3-flash-preview为55.46%。开源模型中Qwen3.5-397B最佳，问题准确率为47.44%。多模态子集更具挑战性，模型表现普遍下降。

**诊断分析**：为探究模型失败原因，研究将参考答案分解为可验证的推理单元序列，并对前沿模型（以Gemini 3 Flash为重点）进行了三项诊断实验：(Q1) 单元执行测试显示，模型在给定正确子问题后，单个步骤的执行准确率很高（文本子集约0.8-0.9），表明原子能力不是主要瓶颈。(Q2) 多步推理实验发现，提供单元答案（而不仅仅是子问题）能显著提升最终答案准确率，尤其是在推理链的中段，这表明可靠地推导和维护正确的中间状态是关键瓶颈。(Q3) 单单元注入实验表明，注入一个带答案的关键中间单元对最终准确率的提升效果，接近提供更长的推理前缀（仅含问题），进一步证实正确中间答案的信号至关重要。

**其他发现**：模型生成的解决方案推理步骤数平均多于参考答案（文本子集：12.20 vs. 10.73；多模态子集：13.86 vs. 11.72），表明推理效率较低且错误积累风险更高。

### Q5: 有什么可以进一步探索的点？

基于论文分析，其局限性及未来研究方向主要体现在以下几个方面。首先，论文诊断出前沿模型在原子级知识/推理上并非主要瓶颈，但在多步推理中可靠地推导和维持正确的中间状态方面存在显著缺陷。这表明模型缺乏对复杂推理链的稳健组合能力，尤其是在需要结合多个先前结果、应用定理或执行多步代数运算的中段步骤中。其次，模型生成的推理步骤通常比参考答案更长，导致推理效率低下，增加了错误累积的风险。这反映了模型在规划最优推理路径和避免冗余步骤方面存在不足。

未来可进一步探索的点包括：1）开发更强的中间状态监督机制，例如设计训练目标，不仅奖励最终答案，还明确奖励正确、紧凑的中间步骤，或引入约束检查来防止状态漂移；2）构建混合系统，将大语言模型与符号计算器、定理证明器或可验证检索工具结合，由外部工具可靠地生成或验证关键中间值，再交由模型进行后续推理，从而弥补其数值/符号推导的脆弱性；3）研究更高效的推理架构或训练方法，例如通过课程学习或强化学习来优化推理步骤的效率和连贯性，减少不必要的步骤；4）将诊断方法扩展到更多领域和任务，深入探究不同模态（如图表与文本结合）下模型状态维持困难的共性原因，并探索针对性的缓解策略。

### Q6: 总结一下论文的主要内容

该论文提出了一个名为CFE的多模态基准测试，旨在评估大语言模型在超过20个STEM领域的推理能力。其核心贡献在于构建了一个基于真实大学作业和考试题目的高质量数据集，并提供了由课程教师编写的参考答案。论文指出，即使是最先进的模型（如Gemini-3.1-pro-preview）在CFE上的总体准确率也仅为59.69%，表明现有模型仍有巨大提升空间。方法上，论文不仅进行常规的准确率评估，还创新性地通过将参考答案分解为推理流程进行诊断分析。主要结论发现，前沿模型虽然常能正确回答中间子问题，但在多步求解中难以可靠地推导和维持正确的中间状态，且其生成的解决方案通常比教师提供的步骤更多，推理效率较低，导致错误积累风险更高。CFE的建立为未来模型开发、训练目标和推理策略提供了一个强调可验证中间监督与高效推理的现实诊断平台。
