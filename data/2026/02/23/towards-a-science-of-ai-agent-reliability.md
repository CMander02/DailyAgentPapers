---
title: "Towards a Science of AI Agent Reliability"
authors:
  - "Stephan Rabanser"
  - "Sayash Kapoor"
  - "Peter Kirgis"
  - "Kangheng Liu"
  - "Saiteja Utpala"
  - "Arvind Narayanan"
date: "2026-02-18"
arxiv_id: "2602.16666"
arxiv_url: "https://arxiv.org/abs/2602.16666"
pdf_url: "https://arxiv.org/pdf/2602.16666v2"
categories:
  - "cs.AI"
  - "cs.CY"
  - "cs.LG"
tags:
  - "Agent Reliability"
  - "Agent Evaluation"
  - "Agent Safety"
  - "Agent Benchmarking"
  - "Agent Performance Metrics"
  - "AI Safety"
  - "Agent Robustness"
  - "Agent Consistency"
relevance_score: 8.5
---

# Towards a Science of AI Agent Reliability

## 原始摘要

AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决AI智能体在现实部署中可靠性评估不足的核心问题。研究背景是AI智能体正被越来越多地用于执行重要任务（如修改代码、管理数据库），尽管标准基准测试的准确率分数显示进展迅速，但实践中智能体仍频繁出现严重故障（如误删数据库、未经授权购物），这表明当前评估方法存在根本局限。

现有方法主要依赖单一的任务平均成功率作为评估指标，这种方法虽然简单易比，但存在明显不足：它无法区分智能体是稳定地在特定任务子集上失败（可预测）还是随机失败；无法区分良性故障（如格式错误）和灾难性故障（如破坏性操作）；也不能评估智能体对输入扰动的敏感性或失败预测能力。这种评估差距使得基准测试表现与现实部署效果严重脱节。

本文要解决的核心问题是：如何系统性地定义和评估智能体的可靠性？作者借鉴安全关键工程领域（如航空、核电）的成熟理念，将可靠性分解为四个关键维度：一致性（多次运行的行为稳定性）、鲁棒性（抗干扰能力）、可预测性（置信度校准和失败预判能力）和安全性（故障严重程度限制）。针对这些维度，论文提出了12个独立于原始准确率的具体度量指标，构建了一个全面的可靠性评估框架，旨在揭示智能体如何执行、退化及失败的内在机制，弥补传统评估的不足。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为三类：可靠性工程基础、AI代理评估方法，以及现有AI评估框架。

首先，在**可靠性工程基础**方面，本文从航空、核电、汽车和工业控制等安全关键领域汲取了核心概念。这些领域长期实践强调可靠性应分解为一致性、鲁棒性、可预测性和安全性四个维度。本文的创新在于首次将这些跨领域的、成熟的工程可靠性原则系统性地引入并重构为AI代理的评估框架，从而为AI代理的可靠性提供了一个超越单一成功率、更为严谨和全面的定义。

其次，在**AI代理评估方法**上，已有许多研究关注了本文所提维度的某些孤立方面。例如，在一致性方面，有研究关注提示敏感性或提出用 pass∧k 替代 pass@k 来衡量稳定性；在鲁棒性方面，有工作研究对输入扰动或提示注入的敏感性；在可预测性方面，机器学习中的校准和选择性预测研究与此相关；在安全性方面，则有大量关于有害内容合规性、谄媚行为的研究。本文与这些工作的关系是**整合与系统化**。它指出现有研究往往将这些现象视为分散的问题，而本文则通过一个统一的可靠性框架将它们联系起来，并揭示了当前评估的空白，例如对故障后果严重性（安全性维度）的关注不足。

最后，在**现有AI评估框架**层面，本文的工作与提供语言模型多指标评估分类法的研究一脉相承，但将焦点专门对准了**AI代理**。本文提出的12个具体度量指标，构成了一个可操作的评估工具集，旨在补充传统的能力基准测试。它与传统评估的核心区别在于，明确将**可靠性**与**能力**分离，论证了高能力并不自动意味着高可靠性，因此需要对两者进行独立且互补的评估。

### Q3: 论文如何解决这个问题？

论文通过构建一个多维度的可靠性评估框架来解决传统单一成功率指标无法揭示AI智能体关键运行缺陷的问题。其核心方法是提出一个包含四个关键维度（一致性、鲁棒性、可预测性、安全性）的十二项具体度量指标，从而对智能体的可靠性进行系统性解构和量化评估。

整体框架和主要模块设计如下：
1.  **一致性（Consistency）**：衡量智能体在相同条件下行为的稳定性，分解为三个子指标：
    *   **结果一致性（C_out）**：衡量同一任务多次尝试中成功或失败的一致性。
    *   **轨迹一致性（C_traj）**：进一步分为分布一致性（C_traj^d，比较动作类型频率）和序列一致性（C_traj^s，比较动作顺序），衡量智能体达成解决方案的路径是否相似。
    *   **资源一致性（C_res）**：量化计算成本和货币成本（如延迟、API调用成本）的波动性。

2.  **鲁棒性（Robustness）**：衡量智能体在遭遇扰动时维持性能的能力，针对三类常见扰动设计指标：
    *   **故障鲁棒性（R_fault）**：评估对基础设施故障（如API超时、服务不可用）的恢复能力。
    *   **环境鲁棒性（R_env）**：评估对操作环境非语义变化的敏感性（如JSON字段重排、日期格式更改、API参数重命名）。
    *   **提示鲁棒性（R_prompt）**：评估对指令的语义等价重述（如重新措辞、跨语言翻译）的稳定性。

3.  **可预测性（Predictability）**：评估智能体表达的信心是否可靠地指示其实际性能，以支持用户决策，包含三个指标：
    *   **校准度（P_cal）**：衡量陈述的信心水平是否与经验成功率匹配。
    *   **区分度（P_AUROC）**：评估信心分数能否成功区分成功与失败。
    *   **Brier分数（P_brier）**：作为一个适当的评分规则，联合衡量校准度和区分度。

4.  **安全性（Safety）**：量化智能体行动可能造成的有害行为的频率和严重性，包含两个指标：
    *   **合规性（S_comp）**：跟踪对预定义约束（如不暴露个人信息、不执行未授权操作）的遵守情况。
    *   **危害严重性（S_harm）**：在违反约束的任务中，衡量后果的严重程度。

**创新点与关键技术**：
*   **系统性解构与操作化**：将抽象的“可靠性”概念具体化为四个可衡量的工程维度及十二项可计算的指标，超越了单一的准确率评估。
*   **关注行为过程与失败模式**：不仅关注“是否成功”（能力），更关注“如何成功与失败”（可靠性），例如通过轨迹一致性评估解决方案路径的稳定性，这对可审计性和过程复现至关重要。
*   **解耦可靠性与能力**：在指标设计中采用归一化（如结果一致性通过p(1-p)归一化方差）和比率比较（如鲁棒性指标计算扰动与正常条件下的准确率比值）等技术，旨在隔离可靠性度量与基础任务能力，从而更纯粹地评估行为的稳定性、可预测性等特性。
*   **差异化的聚合策略**：对四个维度采用不同的聚合与报告策略。一致性、鲁棒性、可预测性三个维度被平均聚合为总体可靠性分数（R）。而安全性维度因其关注尾部风险和严重后果的特性，被单独报告，视为硬约束而非可与其他维度权衡的连续度量，这防止了平均化掩盖关键安全风险。
*   **面向实际部署的考量**：指标设计紧密贴合实际部署中

### Q4: 论文做了哪些实验？

论文在实验部分对AI智能体的可靠性进行了全面评估。实验设置方面，研究评估了来自OpenAI、Google和Anthropic三个提供商的14个模型（涵盖从2024年初到2025年底发布的多个能力层级），并采用了两种互补的基准测试：GAIA（包含165个任务，涉及网页浏览、文件操作和多步推理）和τ-bench（一个包含26个已验证任务的客户服务模拟基准）。评估协议包括多轮运行（K=5次）以测量一致性、提示扰动（J=5个语义等效的释义）、故障注入（故障概率pfault=0.2）、环境扰动（工具接口格式变更）、置信度估计（事后自我评估）以及安全性分析（基于LLM的错误严重性/合规性评估）。

对比方法主要围绕所提出的四个可靠性维度（一致性、鲁棒性、可预测性、安全性）及其下的十二个具体指标展开，分析了不同模型在这些维度上的表现，并考察了可靠性随模型发布时间、能力以及任务难度的变化趋势。

主要结果与关键数据指标包括：
1.  **整体可靠性进展有限**：尽管经历了18个月的模型发展，整体可靠性仅显示出小幅改善。τ-bench显示出适度增益，而GAIA的改进微乎其微。
2.  **一致性**：所有模型的结果一致性（outcome consistency）普遍较低（见图表），即智能体能够解决的任务也常常无法一致地完成。资源一致性（尤其是token和计算使用量）在运行间方差很高，在GAIA上尤为明显。
3.  **鲁棒性**：故障鲁棒性和环境鲁棒性在大多数模型上显示出天花板效应。然而，提示鲁棒性（对表面指令改写的敏感性）仍是关键区分因素，且模型间差异很大。
4.  **可预测性**：**校准性**（预测置信度与准确性的对齐）在近期前沿模型中明显改善，特别是Claude模型。**判别性**（区分正确与错误预测的能力）趋势在基准间出现分歧：在τ-bench上普遍改善，在GAIA上却大多恶化。
5.  **安全性**：近期前沿模型的违规率显著降低，各提供商能力最强的模型获得了最高的合规性分数。危害严重性分数普遍较高，表明发生的违规大多属于低到中度严重性。
6.  **模型类型分析**：可靠性并未随模型能力均匀提升。虽然校准性、鲁棒性和安全性通常随模型规模（同系列内）提升而改善，但一致性常呈现相反模式：较小模型往往能达到与较大模型相当或更高的一致性。推理模型通常（但并非始终）比非推理模型更可靠。
7.  **任务难度影响（GAIA）**：一致性通常随任务难度单调变化（而非U型）。资源一致性在复杂任务上普遍下降。可预测性（校准性和判别性）在更难任务上平均略有下降。鲁棒性指标与难度没有系统性关系。

### Q5: 有什么可以进一步探索的点？

该论文提出了一个多维度的可靠性评估框架，但仍有进一步探索的空间。其局限性在于：当前评估主要基于静态基准测试，未能充分模拟真实世界中动态、开放的环境；所选的十二个指标虽具启发性，但尚未经过大规模实证验证，其普适性和权重分配需进一步研究；此外，评估对象集中于现有模型，未深入探讨不同架构（如模块化与端到端）对可靠性的影响。

未来研究方向可包括：第一，开发动态环境下的持续评估方法，引入对抗性干扰和长周期任务，以测试智能体的适应性与退化模式；第二，将可靠性指标与具体应用场景（如医疗、金融）结合，定义领域特定的安全边界与错误严重性分级；第三，探索可靠性背后的可解释性机制，例如通过归因分析识别导致不一致或不可预测行为的内部因素；第四，研究“可靠性对齐”技术，在训练中显式优化多维指标，而不仅仅是任务成功率。这些改进有望推动智能体从基准性能向实际可信部署迈进。

### Q6: 总结一下论文的主要内容

该论文针对当前AI智能体评估体系过于依赖单一成功率指标的问题，提出了一套更全面的可靠性评估框架。核心贡献在于将智能体可靠性分解为一致性、鲁棒性、可预测性和安全性四个维度，并设计了十二个具体度量指标，从而系统性地刻画智能体在运行稳定性、抗干扰能力、故障模式可预见性及错误严重程度边界等方面的表现。方法上，论文基于安全关键工程思想构建评估体系，并在两个互补基准上对14个模型进行了实证评估。主要结论表明，尽管模型在传统能力指标上进步显著，但其可靠性提升有限，暴露出当前智能体在实际部署中仍存在持续且隐蔽的缺陷。该研究的意义在于为理解和改进智能体的实际行为模式提供了细粒度分析工具，有助于推动AI系统向更可靠、更可预测的方向发展。
