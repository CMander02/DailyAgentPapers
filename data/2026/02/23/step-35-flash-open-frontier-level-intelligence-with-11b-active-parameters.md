---
title: "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters"
authors:
  - "Ailin Huang"
  - "Ang Li"
  - "Aobo Kong"
  - "Bin Wang"
  - "Binxing Jiao"
  - "Bo Dong"
  - "Bojun Wang"
  - "Boyu Chen"
  - "Brian Li"
  - "Buyun Ma"
  - "Chang Su"
  - "Changxin Miao"
  - "Changyi Wan"
  - "Chao Lou"
  - "Chen Hu"
  - "Chen Xu"
  - "Chenfeng Yu"
  - "Chengting Feng"
  - "Chengyuan Yao"
  - "Chunrui Han"
date: "2026-02-11"
arxiv_id: "2602.10604"
arxiv_url: "https://arxiv.org/abs/2602.10604"
pdf_url: "https://arxiv.org/pdf/2602.10604v2"
categories:
  - "cs.CL"
  - "cs.AI"
tags:
  - "Agent 架构"
  - "Agent 推理"
  - "工具使用"
  - "Agentic 强化学习"
  - "Agent 自演化"
  - "Agent 评测/基准"
  - "计算效率"
  - "稀疏专家混合模型 (MoE)"
  - "多轮交互"
relevance_score: 9.5
---

# Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters

## 原始摘要

We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决开源大语言模型在迈向“前沿级”智能体应用时所面临的两个核心矛盾：**复杂推理能力不足**与**计算效率低下**。研究背景是，尽管开源模型在可验证任务上已快速逼近闭源前沿系统，但在需要多轮交互、长上下文和深度推理的智能体任务中，其表现仍存在显著差距，且现有模型的计算开销和延迟使其难以在资源受限或需要实时响应的工业环境中部署。

现有方法的不足主要体现在两方面：首先，在模型能力上，当前的后训练系统存在两难：训练单一通用模型往往牺牲领域专长，而维护多个领域专家模型则导致迭代碎片化和成本不可持续。其次，在训练方法上，尤其是对于混合专家模型，传统的强化学习方法在扩展到长轨迹推理时，会因策略外样本的微小令牌级误差累积而产生高方差梯度，导致优化不稳定，难以实现稳定、可扩展的性能提升。

因此，本文要解决的核心问题是：**如何设计一个兼具“前沿级”智能体能力（如复杂推理、代码、工具使用）与极致推理效率的开放模型**。具体而言，论文通过提出Step 3.5 Flash模型及其配套训练框架来应对这一挑战：在架构上，采用稀疏混合专家设计（196B总参数，仅激活11B）并结合交错滑动窗口/全注意力与多令牌预测，以降低延迟和成本；在训练上，提出一个统一的、可扩展的强化学习框架，通过结合可验证信号与偏好反馈，并引入新的优化方法（如MIS-PO）来确保大规模策略外训练的稳定性，从而实现模型在数学、代码和工具使用等多个领域持续、一致的自我改进，最终目标是在保持极高推理效率的同时，达到与GPT-5.2、Gemini 3.0 Pro等闭源前沿模型相媲美的智能体性能。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为以下几类：

**1. 稀疏专家混合模型（MoE）与高效推理方法：**
相关工作包括DeepSeek-MoE、Mixtral等模型，它们通过激活部分参数来提升模型容量同时控制推理成本。本文的Step 3.5 Flash同样采用稀疏MoE架构（196B总参数，11B激活参数），但其核心区别在于专门针对智能体交互的延迟和成本进行了优化，引入了**交错滑动窗口/全注意力机制（3:1）**和**多令牌预测（MTP-3）**，旨在减少多轮智能体交互的延迟，这是对传统MoE模型在智能体场景下的针对性增强。

**2. 智能体能力与强化学习训练框架：**
相关研究涉及使用人类反馈强化学习（RLHF）或直接偏好优化（DPO）来对齐模型。本文与之的关系在于同样采用强化学习框架来提升模型在数学、代码和工具使用等方面的能力。其关键区别在于提出了一个**可扩展的强化学习框架**，该框架结合了**可验证的信号**（如代码执行结果、数学证明步骤）与偏好反馈，并声称能在大规模离策略训练下保持稳定，从而实现跨领域的持续自我改进，这比单纯依赖人类偏好的方法更具可扩展性和稳定性。

**3. 前沿模型性能评测：**
相关工作包括各类基准测试，如LiveCodeBench、IMO-AnswerBench等，用于评估模型在代码、数学和智能体任务上的能力。本文与这些工作的关系是直接在这些基准上评估模型性能，并将其与GPT-5.2 xHigh、Gemini 3.0 Pro等前沿模型进行对比，以证明其达到了“前沿水平”。本文的贡献在于提供了一个在高效架构下达到可比性能的具体实例，重新定义了“效率前沿”。

**总结而言，** 本文并非孤立创新，而是紧密围绕**高效MoE架构**、**针对智能体优化的训练技术**和**前沿性能评测**这三个领域展开，其核心贡献在于将这些方向有机结合，专门为部署复杂、多轮交互的现实世界智能体提供了一个高效且高性能的解决方案。

### Q3: 论文如何解决这个问题？

论文通过一个系统性的模型-系统协同设计范式来解决智能体时代对推理能力、计算效率和推理延迟的三大核心需求。其核心方法围绕一个稀疏混合专家模型架构展开，旨在以有限的激活参数实现前沿水平的智能。

**整体框架与架构设计：**
模型采用一个45层的稀疏MoE Transformer骨干网络，包含3个稠密层和42个MoE层。总参数量高达1960亿，但通过MoE路由，每个token仅激活8个专家，使得推理时活跃参数控制在110亿，显著降低了计算开销。该架构与一个专门的混合注意力层布局配对，以优化长上下文处理。

**主要模块、组件与关键技术：**

1.  **混合注意力机制**：为了平衡长上下文处理效率与模型能力，论文设计了3:1交错滑动窗口注意力与全注意力的混合布局。具体而言，每四层为一个单元，包含三层滑动窗口注意力（SWA，窗口大小W=512）和一层全注意力（GQA-8）。这种设计大幅降低了预填充和解码阶段的FLOPs。为了弥补纯交错布局带来的性能损失，引入了两项关键增强：**增加SWA层的查询头数量**（从64增至96），以及采用**头门控注意力机制**。后者通过数据依赖的门控机制，有效解决了SWA在输入窗口内无有用信息时无法有效利用注意力权重的问题，其性能优于固定的“汇”令牌方法，且计算开销可忽略。

2.  **稀疏MoE与负载均衡**：模型采用细粒度MoE设计，每层包含288个路由专家和1个共享专家。为了应对专家并行部署中因路由不均衡导致的“掉队者”问题，论文引入了**EP组平衡MoE路由策略**。该策略在标准负载均衡损失之外，增加了一个显式促进专家组间均匀利用的EP级平衡损失，确保了分布式部署下的高吞吐量。

3.  **多令牌预测**：为了加速自回归生成，模型集成了三个轻量级的多令牌预测头，每个头由一个SWA层和一个稠密前馈网络组成，仅增加约0.81B参数。这些头用于预测未来多个位置的token，与推测解码技术协同工作以降低延迟。训练时，首先仅优化第一个MTP头，待骨干网络训练好后，再克隆并联合微调所有MTP头，并采用位置相关的损失重加权来防止对远距离预测的过优化。

4.  **训练稳定性与课程设计**：针对大规模稀疏MoE训练的不稳定性，论文构建了全面的诊断和缓解方案。关键措施包括：为Muon优化器的数值敏感迭代使用混合精度以防止损失尖峰；监控专家侧激活和参数范数以早期检测“死亡专家”；以及对深层MoE层中出现的激活爆炸现象，采用**专家内部激活裁剪**而非权重裁剪，有效稳定了训练。训练课程从广泛的开放域预训练开始，逐步转向代码、PR/问题/提交数据以及工具使用和推理数据，并分阶段将上下文长度从4k扩展到32k，再扩展到128k，以强化长程推理和智能体能力。

**创新点总结：**
*   **模型-系统协同设计**：将推理延迟作为与智能、成本并列的核心约束进行一体化优化。
*   **高效的混合注意力布局**：3:1 SWA/Full交错设计，辅以查询头增强和头门控机制，在极低成本下保持了强大的长上下文能力。
*   **面向部署的MoE优化**：EP组平衡路由策略解决了分布式推理中的负载不均衡瓶颈。
*   **轻量级多令牌预测集成**：与推测解码紧密结合，针对性优化了多轮交互智能体工作流的生成速度。
*   **可扩展且稳定的训练框架**

### Q4: 论文做了哪些实验？

论文实验主要围绕验证模型架构设计的高效性与性能展开。实验设置上，作者训练了不同规模的模型（如30B-A3B和100B-A10B MoE）进行端到端评估，包括预训练、32k长上下文扩展和64k上下文长度的监督微调（SFT）。数据集和基准测试涵盖推理、数学、代码、科学和长上下文任务，具体使用了LongCtx、Sci、Code等内部基准，以及BBH、MMLU、GPQA、MBPP、C-EVAL、CMMLU等公共基准。

对比方法上，重点比较了不同的注意力布局：全注意力（$FFFF$）、交替SWA/全注意力（$S1F1$）、3:1 SWA与全注意力交错布局（$S3F1$）及其增强版（$S3F1$+Head）。同时，还比较了头门控注意力（Head-wise Gate）与固定汇聚令牌（Sink Token）机制。

主要结果方面：在30B规模下，$S3F1$+Head布局在注意力侧预填充和解码的FLOPs分别仅为全注意力布局的约1/2.68和1/2.90，成本显著降低。性能上，$S3F1$+Head在预训练平均分达到55.7，优于$FFFF$的54.1；下游任务中，长上下文（LongCtx）得分28.2，科学（Sci）得分44.0，接近或超过全注意力基线，仅在代码（Code）任务上有小幅下降至18.3。而$S1F1$布局虽在长上下文得分最高（29.6），但FLOPs成本比$S3F1$+Head高出约60%。在100B规模下，头门控注意力机制在多个基准上均优于汇聚令牌，平均性能从62.46提升至64.43（+1.97）。这些实验验证了所提混合注意力、头门控及MoE负载均衡等设计在保持前沿模型性能的同时，显著提升了推理效率。

### Q5: 有什么可以进一步探索的点？

该论文在稀疏专家混合模型与强化学习框架的结合上取得了显著进展，但其探索仍存在局限。首先，模型在复杂、开放域的真实世界任务（如跨领域工具调用和动态环境交互）中的泛化能力尚未充分验证，当前基准测试可能无法完全反映实际部署的挑战。其次，虽然采用了多令牌预测和滑动窗口注意力来降低延迟，但在高并发、长序列的实时交互场景中，推理效率和内存管理仍需进一步优化。

未来研究方向可从三方面展开：一是探索更动态的专家路由机制，使模型能根据任务复杂度自适应调整激活参数规模，进一步提升效率与性能的平衡；二是强化学习框架可引入更细粒度的奖励信号设计，例如对推理链的中间步骤进行验证，以提升训练稳定性和策略可解释性；三是推动“系统级智能”研究，将模型与外部知识库、执行环境更深层集成，实现持续学习和在线适应。此外，开源模型架构与训练数据，促进社区在可复现性和伦理对齐方面的协作，也是关键的发展路径。

### Q6: 总结一下论文的主要内容

本文介绍了Step 3.5 Flash模型，这是一个稀疏混合专家模型，旨在以高效的计算成本实现前沿水平的智能体能力。其核心问题是解决开源模型在复杂推理任务上落后于闭源前沿系统，以及在长上下文智能体任务中存在的效率瓶颈。方法上，模型采用总参数量196B、每令牌仅激活11B参数的MoE架构，并结合3:1比例的滑动窗口注意力与全注意力以及多令牌预测来降低延迟。为了提升能力，论文设计了一个可扩展的强化学习框架，结合可验证信号与偏好反馈，并引入Metropolis独立性采样-过滤策略优化来确保大规模离策略训练的稳定性，从而实现跨数学、代码和工具使用领域的持续自我改进。主要结论显示，该模型在多项智能体、编码和数学基准测试中达到了与GPT-5.2 xHigh等前沿模型相当的性能，例如在IMO-AnswerBench上达到85.4%，在LiveCodeBench-v6上达到86.4%。通过重新定义效率边界，该模型为在现实工业环境中部署复杂智能体提供了高效的基础。
