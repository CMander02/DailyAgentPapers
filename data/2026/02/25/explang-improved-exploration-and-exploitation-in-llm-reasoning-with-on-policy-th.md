---
title: "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection"
authors:
  - "Changjiang Gao"
  - "Zixian Huang"
  - "Kaichen Yang"
  - "Jiajun Chen"
  - "Jixing Li"
  - "Shujian Huang"
date: "2026-02-25"
arxiv_id: "2602.21887"
arxiv_url: "https://arxiv.org/abs/2602.21887"
pdf_url: "https://arxiv.org/pdf/2602.21887v1"
categories:
  - "cs.CL"
tags:
  - "强化学习"
  - "大语言模型推理"
  - "多语言"
  - "探索与利用"
  - "后训练"
  - "Agent 推理"
relevance_score: 7.5
---

# ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection

## 原始摘要

Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决大型推理模型（LRMs）在强化学习后训练中因过度依赖英语思维而导致的探索与利用效率不足的问题。当前研究主要聚焦于英语推理，期望获得最佳性能，这源于模型训练数据以英语为主。然而，这种单一语言依赖忽视了多语言思维在扩展探索空间、提升推理多样性方面的潜在优势，同时也无法满足全球用户对母语思维轨迹的需求。现有方法通常将多语言推理视为阻碍性能的“诅咒”或仅作为服务不同语言背景用户的工具，未能有效利用多语言性来增强模型的核心推理能力。

本文的核心问题是：如何通过一种新颖的训练流程，使模型能够自主选择思维语言，从而在强化学习过程中更高效地进行探索与利用，最终提升整体推理性能。为此，作者提出了ExpLang方法，通过在强化学习中引入策略上的思维语言选择作为动作，扩展了探索空间，并利用非英语优势改善了利用效果。该方法与大多数强化学习算法正交，为利用多语言性改进大型推理模型提供了新视角。

### Q2: 有哪些相关研究？

本文的相关研究主要涉及大型推理模型（LRM）的训练方法、多语言推理以及强化学习（RL）应用等领域，可分为以下几类：

**方法类**：在LRM的RL后训练方面，GRPO等工作通过消除价值模型降低了训练成本，推动了多种新RL算法的发展。本文提出的ExpLang方法在RL过程中引入思维语言选择作为动作，与这些RL算法正交，扩展了探索空间并提升了利用效果。

**多语言推理研究**：现有对LRM多语言性的观点可分为四类：视为性能损害的“诅咒论”、强调适应用户需求的“服务论”、关注跨语言能力迁移的“可转移论”，以及认为非英语思维可能带来优势的“潜在受益论”。本文属于“潜在受益论”，并进一步提出通过策略性多语言思维来改善训练和用户合规性。

**多语言思维控制方法**：当前方法主要包括两种：一是轻量但不稳定的“前缀黑客”技术，即在生成前添加语言特定前缀；二是稳定但可能导致性能下降的“后训练”微调方法。本文结合两者优势，提出了一种稳定且成本高效的思维语言控制方式，通过RL过程中的策略性选择实现多语言思维的灵活运用。

与这些工作相比，ExpLang的创新在于将思维语言选择作为RL中的策略动作，不仅提升了模型在相同训练预算下的性能，还增强了对已见和未见语言的思维语言合规性，为利用多语言性改进LRM开辟了新视角。

### Q3: 论文如何解决这个问题？

论文通过提出一个名为ExpLang的新型LLM后训练流程来解决多语言思维在推理任务中的探索与利用问题。其核心方法是引入“在线策略思维语言选择”机制，将语言选择本身作为强化学习过程中的一个动作，从而扩展探索空间并提升利用效率。

整体框架包含三个关键构建模块。首先，基于现有开源模型和英语数学数据集，通过教师模型生成高质量的多语言思维数据，并使用语言检测和答案验证进行双向过滤，确保数据质量。随后，在数据样本前添加语言选择标签（如`<lang_select>[lang]</lang_select>`），并通过基于LoRA的监督微调对齐基础模型，使其具备对语言选择标签的高依从性，同时避免灾难性遗忘。

核心创新在于第三个模块：在强化学习与验证训练过程中，设计了一个两阶段的奖励函数来引导在线策略的语言选择。奖励函数综合了格式合规性、语言依从性、思维语言多样性、语言Pass@k奖励以及基于规则的验证奖励。在探索阶段（占训练步数的前1/4），设置多样性奖励权重，鼓励模型尝试使用训练批次中 underrepresented 的语言进行思维，从而扩展探索空间并生成多样化的推理轨迹。在利用阶段（占后3/4），则关闭多样性奖励，启用Pass@k奖励，对同一思维语言下产生正确答案的响应进行奖励，从而引导模型收敛到表现最佳的一种或少数几种思维语言上，实现高效利用。

该方法的关键技术特点包括：1）将语言选择作为在线策略动作，而非离线策略的强制指定，提升了训练稳定性；2）通过两阶段动态调整的奖励机制，平衡了探索与利用；3）在探索阶段禁用KL散度损失以增强输出多样性，在利用阶段再启用以减缓语言选择的过度集中。这种设计使得模型能够自主、动态地选择思维语言，有效利用多语言潜力提升数学推理性能，且方法与多数RL算法正交，为改进大型推理模型提供了新视角。

### Q4: 论文做了哪些实验？

实验设置方面，研究采用Qwen3模型系列（4B用于训练，32B用于数据生成），并使用OpenR1-Math-220k数据集进行训练。训练策略包括基于LLaMA-Factory的LoRA微调（SFT阶段）和基于VeRL的改进GRPO算法（RLVR阶段）。生成长度限制为4096以控制成本。

数据集与基准测试包括三个数学测试集：MATH-500（500个竞赛数学题）、AIME 2025（30个AIME考题）和OlymMATH的en-easy子集（100个AIME级别题目）。对比方法设置了两个基线：朴素基线（Qwen3-4B直接使用GRPO训练）和受控基线（先进行仅英文思维链的LoRA SFT，再进行GRPO训练）。

主要结果如下：在自动思维语言选择设置下，ExpLang方法在多数测试集上取得了最高的准确率和Pass@k值。例如，在MATH-500上，4B模型的ExpLang准确率达91.5%，高于受控基线的91.0%和朴素基线的91.4%；在AIME-2025上，ExpLang准确率为31.9%，高于基线的26.4%和30.8%。关键指标包括：准确率（Acc）、Pass@k、思维令牌数（Tokens）和语言遵从率（Compliance）。ExpLang在RL阶段展现出更高的性能增益，如在MATH-500上，ExpLang的准确率相比初始模型提升了19.4个百分点，而受控基线仅提升12.4个百分点。同时，ExpLang显著减少了思维令牌使用量（如MATH-500上为1263.3，低于基线的1416.9和1440.0），并实现了接近饱和的已见语言遵从率（如MATH-500上达99.9%）。在未见语言（如印尼语、希伯来语）的泛化测试中，ExpLang也表现出更高的准确率和遵从率。消融分析证实了LoRA SFT、多样性增强探索阶段和语言Pass@k利用阶段的必要性，且KL损失的影响较小。分析还显示，ExpLang通过多语言探索扩展了策略熵，并更有效地利用了非英语思维的优势。

### Q5: 有什么可以进一步探索的点？

该论文的局限性在于仅验证了ExpLang方法在GRPO算法上的有效性，虽提及可扩展至其他RL算法（如Dr. GRPO、DAPO等），但缺乏系统性的跨算法对比实验，其通用性有待进一步验证。此外，研究主要关注推理任务，未探索在多模态、创造性生成等更复杂场景中多语言思维选择的潜力。

未来研究方向可从三方面展开：一是将ExpLang与更广泛的RL算法及课程学习、对抗训练等技术结合，以强化探索-利用平衡；二是深入分析多语言思维在特定任务（如数学证明、代码生成）中的优势机制，可引入认知科学理论解释语言选择如何影响推理路径；三是探索动态语言策略，允许模型根据问题复杂度自适应调整语言偏好，而非固定模式。此外，可研究低资源语言的思维链效果，以提升模型的文化包容性与实用性。

### Q6: 总结一下论文的主要内容

本文提出ExpLang，一种新颖的大语言模型推理后训练框架，通过引入策略上的思维语言选择机制，利用多语言优势来增强强化学习过程中的探索与利用。核心问题是现有大型推理模型主要依赖英语进行思维训练，忽视了多语言思维在提升推理多样性和满足全球用户需求方面的潜力。方法上，ExpLang将思维语言选择作为强化学习中的可学习动作，允许模型在训练中动态选择不同语言进行内部推理，从而扩展探索空间并利用非英语语言可能带来的认知优势。实验表明，在相同训练预算下，该方法持续超越仅用英语训练的基线模型，并在已见和未见语言上均保持较高的思维语言遵循度。分析揭示模型经历了“英语-多语言-英语”的思维语言偏好转变，其中多语言探索阶段对性能提升起主导作用。该方法与多数强化学习算法正交，为通过多语言性改进推理模型开辟了新视角。
