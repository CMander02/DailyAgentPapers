---
title: "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs"
authors:
  - "Junxiao Yang"
  - "Jinzhe Tu"
  - "Haoran Liu"
  - "Xiaoce Wang"
  - "Chujie Zheng"
  - "Zhexin Zhang"
  - "Shiyao Cui"
  - "Caishun Chen"
  - "Tiantian He"
  - "Hongning Wang"
  - "Yew-Soon Ong"
  - "Minlie Huang"
date: "2025-05-18"
arxiv_id: "2505.13529"
arxiv_url: "https://arxiv.org/abs/2505.13529"
pdf_url: "https://arxiv.org/pdf/2505.13529v2"
categories:
  - "cs.AI"
  - "cs.CL"
  - "cs.LG"
tags:
  - "Reasoning"
  - "Reliability"
  - "Confidence Calibration"
  - "System 2 Reasoning"
  - "Model Training"
  - "Factual Accuracy"
relevance_score: 5.5
---

# BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs

## 原始摘要

Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决大型推理模型（LRMs）在事实性任务中表现出的过度自信和不可靠问题。当前，LRMs在数学和逻辑推理方面展现出强大能力，但它们在面对知识边界之外或不确定的问题时，很少承认无知或回答“我不知道”，反而经常在过度推理后产生错误答案，并表现出不当的自信，这严重影响了模型的事实可靠性。

研究背景在于，尽管LRMs性能提升，但其可靠性并未同步改善，事实性幻觉率上升，在事实任务上的帮助性反而下降。现有方法主要关注提升模型的推理能力，但缺乏对模型“知之为知之，不知为不知”这种边界意识的培养。现有LRMs普遍存在两个缺陷：一是无法识别自身知识盲区，倾向于编造答案；二是其回答不一致，对相似问题有时正确有时错误，降低了整体事实准确性。

本文的核心问题是：如何让LRMs进行可靠的事实推理，即既能准确表达已知知识，又能对未知问题坦然承认无知。为此，论文具体聚焦于由“过度思考”导致的两种病态推理模式：1）“最后一分钟猜测”：模型经过大量推理仍无定论，最后时刻仓促猜测一个答案；2）“二次思考螺旋”：模型最初得出正确答案，却因继续过度分析而推翻正确结论。为了解决这些问题，本文提出了BARREL框架，旨在训练LRMs进行简洁、审慎且边界感知的事实推理，使其在探索足够候选答案后能果断终止推理——已知则自信回答，未知则主动拒绝，从而提升模型的事实可靠性。

### Q2: 有哪些相关研究？

本文的相关工作主要可分为知识边界界定与事实性对齐两大类，与本文提出的BARREL框架既有联系又有区别。

在**知识边界界定**方面，相关研究旨在识别模型可靠知识的范围，常用方法包括置信度校准、内部状态探测和不确定性估计。为处理超出知识边界的问题，许多工作训练模型在不确定时主动拒绝回答，例如生成“我不知道”或提供解释。代表性研究包括R-tuning、Rejection Improves Reliability以及Don't Just Say "I don't know"等。本文BARREL框架与之紧密相关，但更进一步，致力于让大型推理模型生成结构化、可解释的关于其知识边界的推理过程，以纠正其过度思考的病理模式。

在**事实性对齐**方面，相关研究旨在提升模型的事实准确性同时保持其指令遵循能力。方法包括在监督微调或强化学习阶段引入事实性感知、利用基于偏好的优化增强自我评估能力，或使用拒绝感知数据集进行微调。代表性工作如FLAME、Self-Alignment for Factuality以及UALIGN等。这些研究主要针对非推理型大语言模型。本文BARREL框架的独特性在于，它首次系统性地针对大型推理模型在推理过程中出现的“最后一分钟猜测”和“二次思考螺旋”两种病理模式，通过纠正过度思考倾向并构建相应的训练数据，专门提升LRMs的事实可靠性，从而将事实性对齐的研究拓展到了复杂的推理任务领域。

### Q3: 论文如何解决这个问题？

论文通过提出一个名为BARREL的框架来解决大型推理模型（LRMs）在事实性问题中过度自信和错误回答的问题。该框架包含三个核心组件：知识标注、用于监督微调（SFT）的推理轨迹构建，以及基于规则的策略优化（GRPO）阶段。

首先，在**知识标注**阶段，系统通过采样策略判断一个问题对于目标模型是“已知”还是“未知”。具体而言，使用多个少样本提示对每个问题生成多个答案样本，如果至少有一个样本与标准答案匹配，则标记为“已知”，否则为“未知”。这为后续差异化处理奠定了基础。

其次，在**推理轨迹构建**阶段，针对识别出的两种病态推理模式（“最后一刻猜测”和“反复纠结”），为不同类型的问题构建结构化的推理轨迹。对于已知问题，轨迹引导模型先检索并确认正确答案及证据，再审视其他备选答案以进行对比，最后基于坚实证据自信地得出结论。对于未知问题，轨迹则引导模型探索可能的答案-证据对，但在缺乏足够支持时，明确承认不确定性并输出谨慎的拒绝（如“我不知道”）。这些轨迹由GPT-4根据详细指令生成，形成长链思维（Long-CoT）风格的推理过程，随后用于对目标模型进行监督微调（SFT），使其学会遵循这种边界感知的推理模式。

最后，在**GRPO阶段**，设计了一个基于规则的奖励函数来进一步强化模型的行为。该函数根据模型生成的回答给予不同奖励：对正确答案给予高奖励（r_c），对有效的拒绝回答给予中等奖励（r_s），对错误或幻觉输出给予低奖励（r_w），且满足 r_c > r_s > r_w。这种设计激励模型在不确定时诚实承认知识边界，而非做出无根据的猜测。随后，采用分组强化策略优化（GRPO）方法，基于此奖励函数对模型进行微调，优化其策略以生成更可靠、边界意识更强的推理和答案。

整体框架的创新点在于：1）明确区分问题的知识状态（已知/未知），并据此构建针对性的推理轨迹以纠正特定病态模式；2）结合了监督微调（学习结构化推理模式）和基于规则的强化学习（优化事实性与可靠性），形成两阶段训练流程；3）奖励函数设计简单而有效，直接鼓励模型在不确定时承认无知，从而显著提升事实可靠性。实验表明，该方法能将DeepSeek-R1-Distill-Llama-8B的可靠性从39.33%提升至61.48%。

### Q4: 论文做了哪些实验？

实验设置方面，论文主要评估了提出的BARREL框架在提升大型推理模型（LRMs）事实可靠性方面的效果。实验使用了三个模型：DeepSeek-R1-Distill-Llama-8B、DeepSeek-R1-Distill-Qwen-7B和Qwen3-8B。训练数据集包括TriviaQA、SciQ和NQ-Open，分别涵盖通用知识、科学推理和基于网络的问答。评估时，从每个数据集的测试集中采样1000个问题，形成一个包含3000个问题的测试集。

对比方法包括：（1）ICL：为事实任务设计的少样本提示的原始推理模型；（2）ICL-IDK：提示LRMs表达不确定性；（3）Distill：使用DeepSeek-R1的推理路径在训练集上进行监督微调（SFT）；（4）Vanilla GRPO：标准的GRPO实现，不含基于不确定性的奖励或先前的SFT阶段；（5）可靠性增强的GRPO：包含两个变体——Vanilla GRPO w/ Verbal Confidence（使用语言置信度提取）和Vanilla GRPO w/ Probing（使用预测分类器）。

评估指标包括准确率（Acc.）、真实性（Truth.）和可靠性（Rel.）。关键数据指标显示，BARREL显著提升了模型的可靠性。例如，在DeepSeek-R1-Distill-Llama-8B上，BARREL将可靠性从Distill基线的39.33%提升至61.58%（论文摘要中提到的61.48%为四舍五入值），同时准确率保持在40.7%，超过了Distill方法的38.43%。在Qwen3-8B上，BARREL实现了71.46%的可靠性，显著高于Probing基线的58.94%。此外，BARREL在真实性（Truth.）和弃答率（Abstain）方面也有显著改善，例如在DeepSeek-R1-Distill-Llama-8B上，真实性达到70.40%，弃答率为29.70%，表明模型能更恰当地表达不确定性。实验还通过示例分析表明，BARREL训练能有效缓解“最后一刻猜测”和“二次思考螺旋”两种病理推理模式，从而在保持较高准确率的同时，大幅提升事实可靠性。

### Q5: 有什么可以进一步探索的点？

该论文提出的BARREL框架虽在提升模型可靠性方面成效显著，但仍存在若干局限和可拓展方向。首先，其核心在于识别并抑制“过度思考”模式，但未深入探讨模型在知识边界模糊或问题本身存在歧义时的处理机制，未来可研究如何让模型更细致地区分“已知不确定性”与“未知领域”。其次，实验主要基于数学与逻辑推理任务，在开放域事实性问答（如涉及时效性知识或多源冲突信息）中的泛化能力有待验证。结合个人见解，可能的改进包括：引入动态置信度校准机制，使模型能根据问题类型和上下文自适应调整回答的确定性；构建多粒度边界评估数据集，不仅判断“能否回答”，还能评估“回答的可靠程度”；探索将边界感知能力与检索增强生成（RAG）结合，当模型识别自身知识不足时，自动触发外部检索并明确标注答案来源，从而构建更透明、可信的推理系统。

### Q6: 总结一下论文的主要内容

该论文针对大型推理模型在事实性任务中过度自信、错误率高且无法承认未知的问题，提出了BARREL框架以提升其事实可靠性。论文首先定义了LRMs事实可靠性的两个目标：识别已知与未知问题，并准确表达已知知识。通过分析，作者发现了导致错误答案的两种病理性推理模式：“最后一刻猜测”和“二次思考螺旋”，即模型要么在冗长推理后仓促猜测，要么推翻初始正确结论。

为解决这些问题，BARREL方法包含三个阶段：首先通过采样探测模型的知识边界；接着利用监督微调训练模型进行简洁、审慎的推理，在已知问题上保持对正确答案的信心并在探索足够候选后停止推理，在未知问题上主动以“我不知道”回应；最后采用基于组别的相对策略优化，通过设置高（正确答案）、中（承认未知）、低（错误答案）三档奖励来鼓励模型在不确定时拒绝回答，无需已知/未知标签。

实验表明，BARREL显著提升了DeepSeek-R1-Distill-Llama-8B的可靠性（从39.33%升至61.48%），同时保持与蒸馏模型相当的准确率。论文的核心贡献在于揭示了事实性过度思考现象，首次探索了LRMs通过推理承认无知的机制，并强调了中等奖励在促进不确定性感知拒绝中的关键作用，为构建更可靠的系统2推理模型提供了新思路。
