---
title: "Pancake: Hierarchical Memory System for Multi-Agent LLM Serving"
authors:
  - "Zhengding Hu"
  - "Zaifeng Pan"
  - "Prabhleen Kaur"
  - "Vibha Murthy"
  - "Zhongkai Yu"
  - "Yue Guan"
  - "Zhen Wang"
  - "Steven Swanson"
  - "Yufei Ding"
date: "2026-02-25"
arxiv_id: "2602.21477"
arxiv_url: "https://arxiv.org/abs/2602.21477"
pdf_url: "https://arxiv.org/pdf/2602.21477v1"
categories:
  - "cs.MA"
tags:
  - "多智能体系统"
  - "Agent架构"
  - "Agent记忆"
  - "LLM推理服务"
  - "系统优化"
  - "近似最近邻搜索"
  - "GPU加速"
relevance_score: 9.5
---

# Pancake: Hierarchical Memory System for Multi-Agent LLM Serving

## 原始摘要

In this work, we identify and address the core challenges of agentic memory management in LLM serving, where large-scale storage, frequent updates, and multiple coexisting agents jointly introduce complex and high-cost approximate nearest neighbor (ANN) searching problems. We present Pancake, a multi-tier agentic memory system that unifies three key techniques: (i) multi-level index caching for single agents, (ii) coordinated index management across multiple agents, and (iii) collaborative GPU-CPU acceleration. Pancake exposes easy-to-use interface that can be integrated into memory-based agents like Mem-GPT, and is compatible with agentic frameworks such as LangChain and LlamaIndex. Experiments on realistic agent workloads show that Pancake substantially outperforms existing frameworks, achieving more than 4.29x end-to-end throughput improvement.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决大语言模型（LLM）智能体（Agent）服务中，**代理记忆（Agent Memory）管理所引发的核心性能瓶颈问题**。

**研究背景**：随着基于LLM的智能体（如任务规划、多轮对话、多智能体协作等）日益复杂，它们需要在执行过程中持续记录和检索外部知识、行动历史、用户画像等动态信息，即维护一个“代理记忆”。这不同于静态知识库的检索增强生成（RAG），代理记忆是高度动态的，需要在LLM推理过程中频繁进行近似最近邻（ANN）搜索以检索相关记忆，并插入新生成的记忆项。

**现有方法的不足**：当前实现主要存在三大缺陷。首先，现有向量数据库或索引系统要么针对静态索引优化，要么采用为传统数据库设计的批量更新策略，无法高效处理代理记忆特有的、频繁且细粒度的更新与搜索交错的工作负载，导致随着记忆规模增大，内存操作开销（主要是ANN搜索）急剧上升，可占总执行时间的82%以上。其次，在多智能体场景下，为每个智能体维护独立索引的传统方法效率低下，上层粗搜索需要遍历所有智能体的索引，下层细搜索则因忽略不同智能体访问模式的差异而导致集群组织失准和搜索低效。最后，现有系统缺乏对异构计算平台（GPU-CPU）的有效协同利用，要么完全驻留GPU（受限于显存），要么仅支持静态索引的GPU缓存，无法适应代理记忆频繁更新的动态性。

**本文要解决的核心问题**：因此，本文的核心目标是设计一个高性能的、面向多智能体LLM服务的内存管理系统，以应对由**大规模存储、频繁更新和多智能体共存**共同引入的复杂且高成本的ANN搜索挑战。具体而言，论文提出的Pancake系统旨在通过三项关键技术解决上述问题：1）针对单智能体，设计能利用访问局部性的多级索引缓存和基于访问模式建模的索引维护策略，以优化频繁细粒度更新下的搜索效率；2）针对多智能体，构建统一的混合图索引结构并进行集群级的访问模式感知，以实现跨智能体的协调索引管理与高效搜索；3）利用CPU-GPU协同加速，通过动态的热点集群计算和异步传输机制，在资源受限且需支持在线更新的环境下提升整体性能。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为三类：基于记忆的智能体框架、向量数据库索引技术，以及动态向量数据库方法。

在**基于记忆的智能体框架**方面，相关工作包括 Mem-GPT、A-Mem 等专门提供内存实现的智能体，以及 LangChain、LlamaIndex 等开源框架内置的存储接口。这些工作主要关注功能实现，但其底层的索引和搜索实现效率不高，当内存规模增长时，查询延迟会急剧增加，成为系统瓶颈。本文提出的 Pancake 系统旨在从根本上解决这一性能问题，提供了一个可集成到现有框架中的高效内存系统。

在**向量数据库索引技术**方面，主流方法如倒排文件（IVF）索引、基于图的索引（如 HNSW、Vamana）等，通过构建结构化存储来优化搜索效率。然而，这些技术主要针对像 RAG 这样的静态、只读场景设计，假设向量数据库是静态的且索引一次性构建完成。这与智能体工作负载中内存频繁更新的需求不兼容。

为此，在**动态向量数据库**方面，出现了如 SPFresh（通过原地插入和轻量级本地重平衡避免全局重建）和 Quake（使用分层聚类结构并根据访问频率自适应分裂聚类）等技术。但这些设计通常假设更新是周期性、批处理式的，与传统数据库工作负载一致。而智能体记忆服务涉及的是与搜索操作紧密交织、高度频繁的更新，导致这些系统的效率和准确性在智能体场景下会下降。

本文的 Pancake 系统与上述所有工作的核心区别在于，它首次专门针对多智能体 LLM 服务中记忆管理的核心挑战（大规模存储、频繁更新、多智能体共存）进行设计，通过多级索引缓存、跨智能体协调索引管理和 GPU-CPU 协同加速等关键技术，实现了对动态、高并发智能体工作负载的高效支持。

### Q3: 论文如何解决这个问题？

论文通过设计一个名为Pancake的多层内存系统来解决多智能体LLM服务中代理记忆管理带来的大规模存储、频繁更新和多智能体共存所引发的复杂且高成本的近似最近邻（ANN）搜索问题。其核心方法、架构设计和关键技术如下：

**整体框架与主要模块：**
Pancake采用协调的多层设计，主要包括三个关键部分：
1.  **受缓存启发的多层索引编排**：系统为单个智能体设计了三层集群缓存（L0, L1, L2），以解决局部访问操作与底层ANN索引粗粒度集群结构之间的不匹配问题。搜索和更新操作从顶层（L0）开始，L0维护一个记录最常访问的微小集群的表，以保持时间局部性。L1缓存更广泛的邻居向量，L2则形成稳定、粗粒度的结构。系统利用基于有限状态机（FSM）的模型来刻画和预测智能体的记忆访问模式，从而实现基于模式的搜索重排序和预取，优化搜索效率并可能触发早期终止。
2.  **支持多智能体的索引管理机制**：系统通过一个混合图结构来统一管理多个智能体的索引。该图连接了静态记忆和每个智能体的本地记忆，在细粒度索引上避免向量冗余存储，在粗粒度索引上为每个记忆范围维护自己的多层图（类似HNSW）。此外，通过以一定概率在不同记忆范围的图节点间建立“互连”，实现了跨智能体记忆的高效协同搜索。同时，系统为静态记忆中的每个集群关联一个“智能体档案”，记录各智能体在该集群内最近访问的向量ID，从而在后续访问时优化搜索顺序。
3.  **GPU-CPU协同的动态索引管理**：为了充分利用异构计算资源，系统设计了CPU端插入缓冲区与GPU端热点感知缓存的协同机制。GPU动态缓存访问频繁的“热点”集群以加速计算。对于已缓存在GPU的集群，新的插入操作会先暂存于对应的CPU端缓冲区。搜索时，系统并行搜索GPU缓存部分和CPU缓冲区中的新向量，然后合并结果，从而在保持低延迟的同时处理更新。系统还采用异步一致性管理来避免同步数据迁移的开销，并利用GPU加速集群分裂等计算密集型操作。

**创新点：**
*   **面向智能体工作负载的索引缓存与预测**：不同于传统的动态ANN方法，Pancake首次利用FSM对智能体的记忆访问轨迹进行建模，实现了基于语义集群状态和转移模式的访问预测，从而支持智能的搜索重排序、预取和早期终止，显著提升了搜索效率。
*   **跨智能体的统一索引与协同搜索**：通过创新的混合图结构和受控的跨图连接，系统在保持各智能体记忆独立性的同时，实现了高效的跨记忆范围搜索，解决了多智能体环境下索引隔离与协同访问的矛盾。
*   **异构硬件感知的动态内存管理**：系统通过热点感知缓存、CPU插入缓冲区与异步一致性维护的紧密结合，在GPU内存受限（尤其是与LLM推理引擎共置时）的场景下，智能地将计算负载在CPU和GPU之间动态分配与迁移，实现了内存高效的热点加速和低延迟的在线更新。

### Q4: 论文做了哪些实验？

论文在混合CPU-GPU服务器上进行了实验，硬件配置包括64核AMD EPYC 9534处理器和8个NVIDIA H100 GPU（80GB内存）。实验使用了多样化的代理数据集，包括多轮人机对话（UltraChat、UltraFeedback）、长链数学推理（Prm800k、Gsm8k）以及任务导向数据集（APIGen、AgentGym）。评估了多种内存访问模式，如“一次搜索一次插入”和“分步搜索后插入”等。

对比方法分为两类：一是代理服务系统，包括A-Mem、MemGPT、LlamaIndex和LangMem，均集成了基于ANN的内存管理后端；二是独立的向量数据库库，包括Quake、SpFresh和DiskANN。此外，还进行了消融实验，比较了Pancake框架内的两种动态维护策略：IVF-Static（静态初始化）和IVF-Split（集群分裂）。

主要结果显示，Pancake在端到端吞吐量上显著优于现有框架。在单代理场景下，针对四种不同的访问模式，Pancake实现了超过4.29倍的端到端吞吐量提升。关键数据指标包括：与基线系统相比，吞吐量提升倍数明确，突显了其多级索引缓存、跨代理协调管理和GPU-CPU协同加速技术的有效性。

### Q5: 有什么可以进一步探索的点？

该论文提出的Pancake系统在提升多Agent场景下的内存检索效率方面取得了显著进展，但其设计和实现仍存在一些局限性和值得深入探索的方向。

首先，系统对Agent工作负载模式的建模依赖于轻量级启发式FSM构建与合并策略，其模式识别的准确性和泛化能力可能受限。未来可探索更精细的在线学习机制，例如引入小样本学习或对比学习，使系统能自适应地捕捉复杂、演变的访问模式，而不仅依赖固定参数（如 \(N_S\) 和 \(d_{merge}\)）的启发式规则。

其次，在多Agent协同索引管理中，跨内存范围的连接概率 \(ef_{connect}\) 基于静态与私有索引的密度比经验设定，这可能无法适应动态变化的工作负载。一个潜在改进方向是设计一个反馈驱动的自适应调节机制，根据跨范围查询的实际命中率与延迟，动态优化连接策略，以实现更优的效率与召回率平衡。

再者，虽然论文提出了GPU-CPU协同的热点缓存与异步一致性管理，但其优化主要针对检索和插入。对于更复杂的操作（如大规模记忆重组或语义聚类更新），异构计算资源的调度可能面临挑战。未来可研究将计算图或算子融合技术引入内存管理系统，实现检索、更新与重组操作的流水线化和更深度的硬件加速。

最后，系统的评估基于模拟的工作负载，其在实际复杂、长期运行的Agent应用（如持续学习的虚拟助手或开放环境游戏Agent）中的长期稳定性与记忆一致性保障尚未得到充分验证。探索如何在该架构下集成记忆压缩、遗忘机制或因果追溯能力，以支持可持续的大规模Agent系统，是一个重要的未来方向。

### Q6: 总结一下论文的主要内容

该论文针对多智能体LLM服务中的记忆管理核心挑战，提出了一种名为Pancake的分层记忆系统。问题在于现有向量数据库难以高效处理智能体记忆所需的大规模存储、频繁更新和多智能体共存带来的复杂近似最近邻搜索开销。Pancake的方法融合了三项关键技术：为单个智能体设计的多级索引缓存，利用访问模式建模优化索引构建；为多智能体协调的混合图索引结构，统一管理不同智能体的索引以减少冗余遍历；以及动态的CPU-GPU协同加速机制，通过热点集群计算和异步传输应对频繁更新。实验表明，Pancake能显著提升端到端吞吐量，相比现有框架达到4.29倍以上的性能提升，并将记忆操作时间占比大幅降低。其核心贡献在于首次为多智能体应用设计了性能导向的记忆管理系统，并提供了易于集成到现有框架的接口。
