---
title: "Document Reconstruction Unlocks Scalable Long-Context RLVR"
authors:
  - "Yao Xiao"
  - "Lei Wang"
  - "Yue Deng"
  - "Guanzheng Chen"
  - "Ziqi Jin"
  - "Jung-jae Kim"
  - "Xiaoli Li"
  - "Roy Ka-wei Lee"
  - "Lidong Bing"
date: "2026-02-09"
arxiv_id: "2602.08237"
arxiv_url: "https://arxiv.org/abs/2602.08237"
pdf_url: "https://arxiv.org/pdf/2602.08237v2"
categories:
  - "cs.CL"
tags:
  - "强化学习"
  - "长上下文处理"
  - "无监督学习"
  - "能力增强"
  - "文档重建"
  - "奖励设计"
  - "基准评测"
relevance_score: 7.5
---

# Document Reconstruction Unlocks Scalable Long-Context RLVR

## 原始摘要

Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决大语言模型（LLM）在长上下文理解和处理能力训练中，对昂贵人工标注或强大教师模型监督的依赖问题，提出一种可扩展的无监督强化学习训练范式。

研究背景是，随着LLM向需要处理海量现实世界数据的智能体发展，其核心挑战已从局部推理转向全局上下文处理。尽管基于可验证奖励的强化学习（RLVR）在数学、编程等领域的逐步推理上取得了突破，但它在处理数万token的长文档时存在局限：擅长逐步推理的模型往往难以在长上下文中保持信息检索与合成的全局连贯性，即存在“中间迷失”等现象。当前，利用RLVR提升模型长上下文能力的主流方法，严重依赖于由人类专家或前沿（常为闭源的）教师模型提供的标准答案或显式评估准则。这造成了显著的扩展瓶颈：长上下文理解的需求是普遍的，但训练所需的人工标注问答对成本极高，难以达到强化学习所需的数据规模；同时，依赖教师模型可能引入偏见或限制学生模型的能力上限。

因此，本文要解决的核心问题是：如何摆脱对昂贵外部监督的依赖，开发一种可扩展的无监督方法，来有效提升LLM的长上下文能力。为此，论文提出基于文档重构的无监督RLVR框架。其核心思想是利用长文档自身固有的叙事流和逻辑序列结构作为内部监督信号。具体方法是在长文档中随机用占位符掩码若干段落，然后让LLM通过强化学习训练，从一组打乱的候选段落中正确识别并排序这些缺失的段落以重构原文。由于原始文档即提供了真实答案，由此产生的奖励既是可验证的，又是完全自动化的。该训练机制迫使模型超越浅层的模式匹配，去学习全局叙事连贯性和长程结构依赖，从而提升长上下文性能。论文通过在RULER和LongBench v2等基准上的实验验证了该方法的有效性。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为两大类：强化学习可验证奖励（RLVR）和长上下文LLM训练。

在**强化学习可验证奖励（RLVR）**方面，已有研究利用数学、编程等具有明确正确答案的任务，通过结果奖励来监督和提升LLM的推理能力。这类工作（如OpenAI的o1系列和DeepSeek模型）通常关注自包含的推理任务，旨在引导模型发现正确的内部推理轨迹。本文与这类工作的核心关系在于，同样采用了基于结果奖励的强化学习范式（如GRPO算法）。然而，关键区别在于：传统RLVR研究依赖于人工或强大教师模型提供的黄金标准答案或评估准则，成本高昂；而本文提出了一种**无监督**方法，通过文档重建任务自动生成可验证的奖励，从而摆脱了对昂贵标注的依赖。

在**长上下文LLM训练**方面，现有方法主要依赖于使用合成数据进行监督微调，例如通过填充无关段落、打乱文档顺序或插入不相关文本来人为增加序列长度。这些方法往往引入的是表面的复杂性，难以产生高质量、具有挑战性的训练样本，从而限制了模型长上下文能力的提升。本文与这类工作的目标一致，即增强LLM的长上下文处理能力。但本文的创新之处在于，它没有采用合成数据扩展上下文这种常见策略，而是提出了一种基于强化学习的、原则性的训练方法。具体而言，本文通过让模型重建被部分替换的长文档来学习把握全局叙事连贯性，为长上下文训练提供了更有效的、可扩展的监督信号。

### Q3: 论文如何解决这个问题？

论文通过一种名为“文档重建”的无监督强化学习范式来解决长上下文能力提升问题，其核心是让大语言模型在无需人工标注或教师模型监督的情况下，学习理解和重建长文档的全局叙事连贯性。

**整体框架与流程**：首先，给定一个由多个段落组成的长文档，系统会随机选择其中若干个段落并将其替换为带有标识符的占位符（如 `<CHUNK_i>MISSING</CHUNK_i>`），从而生成一份“损坏”的上下文。同时，这些被掩码的段落会被打乱顺序，作为候选选项提供给模型。模型的任务是基于损坏的上下文，通过推理生成一个有序的选项序列，以正确重建原始文档的顺序。例如，若掩码了四个段落，模型需输出类似 `{B, A, D, C}` 的序列。这一设计将长上下文理解问题转化为一个序列决策任务，迫使模型必须捕捉文档的整体叙事流和逻辑一致性才能正确排序。

**关键技术：可验证的奖励函数**：该方法的关键创新在于设计了一个完全自动化、可验证的奖励机制。奖励函数根据模型输出序列与真实序列的匹配程度进行计算：若完全一致，奖励为1；若输出是一个有效排列（即选项集合与真实集合完全相同）但顺序不完全正确，则奖励为正确放置段落的比例（即部分奖励）；若输出无效（如格式错误、选项遗漏或重复），奖励为0。这种分段式奖励结构既鼓励模型追求全局准确，又在未完全正确时提供细粒度反馈，驱动模型逐步优化对整体结构的理解。

**创新点与训练策略**：1) **无监督与自监督**：整个训练信号仅依赖于原始文本自身提供的真实顺序，无需任何外部标注或教师模型，大幅降低了成本。2) **难度可控的课程学习**：通过调整掩码段落数量 \(K\)，可以精确控制样本难度。\(K\) 越大，候选排列空间（\(K!\)）呈指数增长，任务越复杂。研究团队将 \(K\) 作为可调超参数，实现了一种课程学习机制——模型先从较少的掩码段落（学习局部连贯性）开始，逐步增加 \(K\) 以挑战更复杂的全局依赖关系，从而渐进式地构建长上下文理解能力。

**主要模块**：包括文档损坏模块（负责段落选择与掩码）、候选选项生成与打乱模块、基于强化学习的序列生成模型，以及自动奖励计算模块。整个流程形成一个闭环的强化学习环境，使模型通过试错学习文档的深层结构信息，最终在 RULER 和 LongBench v2 等基准上显著提升了长上下文性能。

### Q4: 论文做了哪些实验？

论文在实验部分进行了全面的评估和分析。实验设置方面，核心是采用无监督的文档重建训练范式：在长文档中随机用占位符替换若干段落，让大语言模型通过强化学习从候选段落集中正确识别并排序缺失段落以重建文档，以此提升长上下文能力。

实验在两个广泛使用的基准测试上进行：RULER（合成推理基准）和LongBench v2（现实QA任务）。对比方法主要与未经重建训练的原始骨干模型（Base）进行对比。主要结果如下：在RULER上，该方法带来了显著提升，且随着上下文长度从32K增至128K，增益持续一致；在LongBench v2上，大多数上下文长度上也观察到了适度改进，且这些改进是在未使用任何人工标注的长上下文QA数据下取得的。性能提升在不同骨干架构（Qwen2.5-7B-Instruct-1M和LLaMA-3.1-8B-Instruct）上保持一致。

论文还进行了广泛的消融研究，分析了关键因素对性能的影响：
1.  **奖励设计**：比较了稠密奖励（对部分正确重建给予部分奖励）和稀疏奖励（仅当预测排序完全匹配真实情况时给予奖励）。结果显示，稀疏奖励在LLaMA模型上表现与稠密奖励相似，但在Qwen模型上导致性能显著下降，推测是由于训练过程中正奖励稀疏导致的不稳定。
2.  **数据混合策略**：比较了不同候选段落数量K（2, 4, 6, 8）的混合比例。主要实验采用比例3:3:3:5，消融实验采用1:2:2:2。结果表明，方法对不同难度混合比例具有鲁棒性，具体最优比例因模型架构而异（Qwen在1:2:2:2时得分最高，LLaMA在3:3:3:5时最有效）。
3.  **文档长度选择**：比较了选择最长文档、最短文档和随机文档三种策略进行训练。结果显示，使用长文档训练能带来更全面的改进，强调了较长上下文对于重建任务的重要性。
4.  **数据规模效应**：研究了训练数据规模（从0到14,000个样本）对性能的影响。结果表明，模型性能通常随着重建训练样本数量的增加而提高，当数据规模超过4,000个样本时，性能持续增长，表明足够的数据对有效增强长上下文理解至关重要。
5.  **训练策略**：比较了课程式训练（逐步增加K值）与随机打乱顺序训练。结果显示，课程式策略（Shuffle False）在两种骨干模型上都一致优于随机打乱训练。
6.  **任务难度（固定K值）**：研究了固定不同K值（2, 4, 6, 8）单独训练的影响。结果显示，模型性能在不同K值下相对稳定，表明训练目标鼓励的是对全局结构的鲁棒理解，而非过拟合特定难度。中等K值（4和6）表现略优。此外，单K训练对较弱的LLaMA模型有益，但会降低较强的Qwen模型性能，突显了主实验中混合不同K值样本策略的重要性。

关键数据指标包括：在RULER上的平均分数，例如Qwen2.5-7B-Instruct-1M的基线（Base）分数为68.86，经过最佳重建训练后（Shuffle False）提升至72.20；LLaMA-3.1-8B-Instruct的基线分数为58.76，最佳训练后提升至67.12。这些数据具体展示了方法带来的性能增益。

### Q5: 有什么可以进一步探索的点？

该论文提出的文档重建方法虽有效，但仍存在局限性和可拓展方向。首先，方法依赖高质量、结构化的长文档数据，在数据稀缺或噪声较多的领域（如社交媒体、对话记录）应用受限。未来可探索对非结构化或碎片化文本的适应性训练，例如通过段落重组或跨文档信息整合来构建训练资源。其次，不同骨干模型对训练数据的质量和长度敏感度差异显著，需进一步研究模型架构与训练方法的协同优化，例如针对小模型设计更精细的段落采样策略。此外，当前实验数据规模和模型尺寸有限，未来可在更大数据量（如百万级文档）和不同参数规模的模型上验证方法的扩展性，并探究训练效率与长上下文性能之间的平衡。最后，该方法目前侧重于叙事连贯性，未来可结合多模态信息或外部知识库，增强模型在复杂推理、事实核查等任务上的泛化能力。

### Q6: 总结一下论文的主要内容

本文提出了一种无监督强化学习框架，旨在通过文档重建任务来提升大语言模型的长上下文能力。核心问题是避免依赖昂贵的人工标注或教师模型监督，传统RLVR方法需要这些来提供标准答案或评分标准。方法上，论文将长文档中部分段落替换为特殊占位符，让模型通过强化学习从候选段落集中正确识别并排序缺失段落以重建原文，其奖励信号可直接从原始文档验证。实验表明，该方法在RULER和LongBench v2基准上显著提升了多种骨干模型的长上下文性能，且无需人工标注的长上下文QA数据。主要结论是，文档结构本身可作为有效的自监督信号，为可扩展的长上下文训练提供了新范式。
