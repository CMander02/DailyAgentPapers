---
title: "An Empirical Study of Bugs in Modern LLM Agent Frameworks"
authors:
  - "Xinxue Zhu"
  - "Jiacong Wu"
  - "Xiaoyu Zhang"
  - "Tianlin Li"
  - "Yanzhou Mu"
  - "Juan Zhai"
  - "Chao Shen"
  - "Yang Liu"
date: "2026-02-25"
arxiv_id: "2602.21806"
arxiv_url: "https://arxiv.org/abs/2602.21806"
pdf_url: "https://arxiv.org/pdf/2602.21806v1"
categories:
  - "cs.SE"
tags:
  - "Agent 评测/基准"
  - "Agent 安全"
  - "Agent 架构"
  - "多智能体系统"
  - "工具使用"
  - "实证研究"
relevance_score: 7.5
---

# An Empirical Study of Bugs in Modern LLM Agent Frameworks

## 原始摘要

LLM agents have been widely adopted in real-world applications, relying on agent frameworks for workflow execution and multi-agent coordination. As these systems scale, understanding bugs in the underlying agent frameworks becomes critical. However, existing work mainly focuses on agent-level failures, overlooking framework-level bugs. To address this gap, we conduct an empirical study of 998 bug reports from CrewAI and LangChain, constructing a taxonomy of 15 root causes and 7 observable symptoms across five agent lifecycle stages: 'Agent Initialization','Perception', 'Self-Action', 'Mutual Interaction' and 'Evolution'. Our findings show that agent framework bugs mainly arise from 'API misuse', 'API incompatibility', and 'Documentation Desync', largely concentrated in the 'Self-Action' stage. Symptoms typically appear as 'Functional Error', 'Crash', and 'Build Failure', reflecting disruptions to task progression and control flow.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决大型语言模型（LLM）智能体框架中存在的缺陷（bug）问题，这是一个在LLM应用规模化背景下至关重要但尚未被充分研究的领域。随着LLM在智能问答、自动化软件工程等实际应用中的快速发展，单一的模型调用已无法满足复杂、长周期的任务需求，因此催生了集成了记忆、工具和控制逻辑的智能体范式。诸如LangChain、CrewAI等智能体框架应运而生，用于支持任务工作流定义、状态管理和多智能体协调。然而，当这些底层框架出现缺陷时，其影响会向上层系统传播和放大，导致错误执行、资源滥用乃至安全风险，对整个LLM软件供应链构成严重威胁。

现有研究主要聚焦于智能体层面的失败，例如在推理、规划或行动过程中的行为偏差，通常通过行为分析或基准测试进行评估。这些工作虽然增进了对智能体行为及模型局限性的理解，但很大程度上忽视了根植于底层框架本身的缺陷。近期虽有研究开始分析智能体库的缺陷，但其方法（如将拉取请求映射到静态库组件）忽略了智能体动态执行和时序工作流的特性。因此，智能体框架的缺陷在动态的智能体生命周期中如何表现，仍然是一个未被探索的空白。

针对这一不足，本文的核心问题是：**系统性地实证研究现代LLM智能体框架中缺陷的根本原因和可观察症状，并理解它们在智能体完整生命周期中的分布与关联。** 为此，研究收集并手动分析了来自CrewAI和LangChain两个代表性框架的998份缺陷报告，构建了一个涵盖15种根本原因和7种症状的分类体系，并将其映射到“智能体初始化”、“感知”、“自我行动”、“相互交互”和“演化”这五个智能体生命周期阶段。通过这一生命周期导向的视角，论文旨在揭示框架级缺陷的主要来源（如“API误用”、“API不兼容”）、表现形式（如“功能错误”、“崩溃”）及其在任务执行流程中的破坏模式，从而为加强LLM软件供应链的质量提供基础性见解。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为三类：方法类、应用类和评测类。

在方法类研究中，现有工作主要关注智能体层面的失败，例如推理、规划和执行过程中的错误，通常通过行为分析或基准测试来评估。这些研究增进了对智能体行为和模型局限性的理解，但大多忽略了底层框架本身的缺陷。一篇近期研究开始分析智能体库的缺陷，通过将拉取请求映射到库的静态组件（如数据预处理），但该方法忽略了智能体的动态执行和时序工作流。

在应用类研究中，随着大语言模型在智能问答、自动化软件工程等领域的广泛应用，智能体框架（如LangChain）被开发出来以支持任务工作流定义、状态管理和工具协调。然而，框架层的缺陷可能向上层系统传播，导致错误执行和安全风险，但现有研究对此关注不足。

在评测类研究中，缺乏针对智能体框架缺陷的系统性实证分析。本文填补了这一空白，首次从生命周期视角（包括初始化、感知、自行动、互操作和演化五个阶段）对框架级缺陷进行大规模实证研究，构建了包含15类根本原因和7类症状的分类体系，揭示了与现有智能体层面研究不同的缺陷模式（如API误用、不兼容和文档不同步）。

### Q3: 论文如何解决这个问题？

论文通过一个严谨的三步实证研究方法来解决对智能体框架层面缺陷缺乏理解的问题。其核心方法是系统性地收集、筛选和分类来自两个主流开源框架（CrewAI和LangChain）的缺陷报告，并构建一个面向生命周期的分类体系。

整体框架分为三个主要步骤：1）**问题收集**：从GitHub官方仓库全面抓取两个框架的原始问题报告，共获得2773个条目，以减少抽样偏差。2）**缺陷过滤**：采用两阶段过滤确保数据相关性。首先利用GitHub的“bug”标签进行初步筛选，然后通过人工检查剔除文档错误、使用问题等无关报告，最终得到998个与框架缺陷直接相关的有效报告。3）**个体标注**：由两位标注者独立工作，对每个缺陷报告进行标注。

该方法包含两个关键模块/组件：**初始分类法构建**和**大规模标注**。在初始构建阶段，研究者随机抽取100份报告，由两位作者独立识别所涉及的核心框架组件、推断根本原因并记录症状表现，通过讨论达成一致，初步归纳出15类根本原因和7类症状。在大规模标注阶段，将此分类法应用于剩余报告，仅在遇到无法归入现有类别的新模式时才引入新类别，确保了分类的一致性和可扩展性。

其创新点在于：第一，**研究视角的转变**，从关注智能体层面的失败转向深入分析支撑其运行的**框架本身**的缺陷。第二，提出了一个**结构化、面向生命周期的分类体系**，将根本原因和症状映射到智能体生命周期的五个阶段（智能体初始化、感知、自我行动、相互交互、进化），为理解和定位框架缺陷提供了系统化的分析基础。第三，采用了**互补框架选择**（LangChain侧重工具调用与流水线，CrewAI侧重基于角色的多智能体协作），使得研究能够覆盖多样化的框架机制和缺陷模式，结论更具普遍性。

### Q4: 论文做了哪些实验？

本研究对CrewAI和LangChain两个现代LLM智能体框架中的998个错误报告进行了实证分析。实验设置包括对来自这两个框架开源仓库的错误报告进行系统性的收集、筛选和人工标注，构建了一个涵盖15个根本原因和7个可观察症状的分类体系，并按照智能体生命周期的五个阶段（智能体初始化、感知、自我行动、相互交互、进化）进行映射分析。

数据集/基准测试即为从CrewAI和LangChain项目公开问题追踪系统中收集的998个真实错误报告，构成了本次实证研究的基础。

主要对比维度是不同根本原因和症状在生命周期各阶段的分布。核心发现如下：
1.  **根本原因分布**：`API误用`（329/998，32.97%）和`API不兼容`（223/998，22.34%）是最主要的根本原因，合计占比超过55%。其次是`文档不同步`（75/998）、`流式传输不稳定`（67/998）和`配置错位`（53/998）等。
2.  **症状分布**：`功能错误`（781/998）是最常见的症状，表明框架虽在运行但核心逻辑出错。其次是`崩溃`（100/998）和`构建失败`（67/998）。`性能低下`（10/998）和`未报告错误`（4/998）则较为少见。
3.  **生命周期阶段分布**：绝大多数错误集中在“自我行动”阶段（882/998），该阶段涉及任务规划、工具调用和图执行，是`API误用`（289例）和`API不兼容`（211例）的高发区。其他阶段错误数量显著较少。

关键数据指标包括：错误报告总数998；两大主导根本原因`API误用`（329例）和`API不兼容`（223例）合计占比55.3%；主导症状`功能错误`（781例）占比78.3%；错误最集中的“自我行动”阶段案例数（882例）占总数的88.4%。这些结果表明，智能体框架的缺陷主要源于接口相关的执行语义问题，并在核心执行阶段集中爆发。

### Q5: 有什么可以进一步探索的点？

该论文主要对CrewAI和LangChain两个框架的bug进行了实证分类，其局限性在于研究范围相对有限，仅聚焦于两个流行框架，且数据来源为已报告的bug，可能遗漏了未记录或特定场景下的隐性缺陷。未来研究可首先扩大实证范围，纳入更多样化的框架（如AutoGen、Semantic Kernel等）和更复杂的多模态、长周期任务场景，以验证分类的普适性。其次，可深入探索bug的自动检测与修复机制，例如结合静态分析、形式化验证或LLM驱动的代码自修复，提前在开发阶段规避“API误用”和“文档不同步”等高频问题。此外，论文发现bug多集中在“自我行动”阶段，未来可进一步研究如何通过设计更鲁棒的容错架构（如引入检查点、事务回滚或动态工作流调整）来提升agent在异常下的持续运行能力。最后，将bug根因与具体的软件工程实践（如测试用例生成、依赖管理工具改进）相结合，也是推动agent框架走向成熟的关键方向。

### Q6: 总结一下论文的主要内容

该论文对现代LLM智能体框架中的缺陷进行了首次系统性实证研究，填补了现有研究主要关注智能体层面失败、而忽视底层框架级漏洞的空白。研究问题聚焦于识别和分析智能体框架中的错误根源、表现症状及其在智能体生命周期中的分布。

方法上，作者收集了CrewAI和LangChain两个流行框架的998份错误报告，通过定性分析构建了一个包含15种根本原因和7种可观察症状的分类体系，并映射到智能体生命周期的五个阶段：智能体初始化、感知、自主行动、相互交互和进化。

主要结论显示，框架错误主要源于“API误用”、“API不兼容”和“文档不同步”三大原因，且高度集中在“自主行动”阶段。其症状主要表现为“功能错误”、“崩溃”和“构建失败”，这些症状严重破坏了任务执行流程和控制流。该研究的分类体系为框架开发者提供了明确的缺陷预防和修复方向，对提升LLM智能体系统的可靠性和可维护性具有重要意义。
