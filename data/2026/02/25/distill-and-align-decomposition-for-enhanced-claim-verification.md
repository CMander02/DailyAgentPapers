---
title: "Distill and Align Decomposition for Enhanced Claim Verification"
authors:
  - "Jabez Magomere"
  - "Elena Kochkina"
  - "Samuel Mensah"
  - "Simerjot Kaur"
  - "Fernando Acero"
  - "Arturo Oncevay"
  - "Charese H. Smiley"
  - "Xiaomo Liu"
  - "Manuela Veloso"
date: "2026-02-25"
arxiv_id: "2602.21857"
arxiv_url: "https://arxiv.org/abs/2602.21857"
pdf_url: "https://arxiv.org/pdf/2602.21857v1"
categories:
  - "cs.AI"
  - "cs.CL"
  - "cs.LG"
tags:
  - "强化学习"
  - "任务分解"
  - "语言模型微调"
  - "信息验证"
  - "推理"
relevance_score: 5.5
---

# Distill and Align Decomposition for Enhanced Claim Verification

## 原始摘要

Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决复杂声明验证任务中，分解质量与验证器对齐之间的协同优化难题。随着大语言模型在关键领域广泛应用，验证其生成文本的事实准确性变得至关重要。当前主流方法是“先分解后验证”的流程，即将复杂声明拆分为可验证的子声明，再逐一核查。然而，现有方法存在显著不足：分解器可能产生过度碎片化、上下文丢失或去语境化不足的噪声子声明，从而损害检索和下游验证性能；同时，分解器和验证器通常被独立优化，导致生成的子声明在信息原子性（atomicity）层面上可能与验证器的处理能力不匹配。以往研究往往孤立地处理分解质量或验证器对齐问题，例如通过复杂多阶段流程提升质量（增加成本和延迟），或仅决策何时分解而不改善分解质量，缺乏一个统一、高效的目标来协同优化两者。

本文的核心问题是：如何设计一种计算高效的训练框架，能够联合优化分解质量与验证器对齐，从而提升整体声明验证的准确性和效率。为此，论文提出了一种强化学习方法，通过组相对策略优化（GRPO）和多目标奖励机制，将结构化顺序推理、教师模型蒸馏示例的监督微调，以及平衡格式合规性、验证器对齐和分解质量的奖励相结合，使得较小规模的语言模型也能在声明验证任务上达到先进水平。

### Q2: 有哪些相关研究？

本文的相关研究主要围绕复杂声明验证中的分解方法展开，可分为方法类和应用/评测类。

在方法类研究中，现有工作主要针对分解的不同目标进行优化，例如原子性、可验证性、去语境化和效率提升。这些方法通常旨在评估大语言模型的事实性，而非直接优化验证准确性，导致两大局限：验证器未对齐（分解目标未针对验证器优化）和分解质量不足（可能引入噪声降低验证准确率）。近期研究尝试分别解决这些问题：为提升分解质量，有工作专注于生成分子事实（即最小化、完全去语境化的子声明），或通过多阶段流程处理选择、消歧和提取；但这些方法未考虑验证器对齐，且常依赖闭源模型的复杂流程。另一方面，有研究通过PPO训练独立策略，依据验证器置信度决定是否分解，以解决验证器未对齐问题，但仅优化分解时机而未改进分解器本身。

本文与上述工作的区别在于，首次通过强化学习联合优化分解质量与验证器对齐。具体而言，本文提出将分解重构为序列推理任务，并在单一模型调用中结合结构化推理步骤；同时设计多目标奖励函数，整合验证器信号与分解质量检查表，使用GRPO进行训练。相比仅关注分解质量或仅调整分解时机的先前方法，本文实现了两者的协同优化，从而在验证准确率和人工评估的分解质量上均取得显著提升。

### Q3: 论文如何解决这个问题？

论文通过一个三阶段的强化学习框架来解决复杂声明验证中的分解与对齐问题。核心方法是将声明分解重构为一个四步顺序推理任务，并采用“蒸馏-对齐”的训练策略，联合优化分解质量与验证器对齐。

**整体框架与主要模块**：
1.  **结构化顺序推理**：分解器对每个目标句子执行四步推理：a) 声明检测（判断句子是否包含可验证命题）；b) 去语境化（重写句子使其自包含、无歧义）；c) 关系识别（识别逻辑或话语关系）；d) 声明提取（将句子拆分为最小的事实单元）。输出包含在 `<think>` 标签内的推理步骤和 `<output>` 标签内的子声明。
2.  **监督微调预热**：首先从更大的教师策略模型蒸馏分解示例，用于初始化学生策略模型。这解决了强化学习的冷启动问题，使学生模型在强化学习训练前就具备指令遵循、输出格式化和基本分解能力。
3.  **基于GRPO的强化学习对齐**：将问题建模为单步马尔可夫决策过程，采用组相对策略优化（GRPO）算法对学生策略进行微调。GRPO通过为每个输入采样多个输出，并计算每个样本奖励相对于其组内平均奖励的优势，在输入复杂度变化时提供更稳定的梯度，且无需单独的评论家网络，计算高效。

**关键技术细节与创新点**：
*   **多目标奖励函数**：设计了三个互补的奖励项来指导策略优化：
    *   **格式奖励**：确保输出结构（如标签、列表解析）正确，采用提供部分信用的软奖励而非二元判断，以提供更丰富的学习信号。
    *   **验证器奖励**：使用下游验证器的预测准确性作为代理信号，以揭示验证器偏好的子声明原子性级别。论文探索了稀疏（基于二元准确率）和稠密（基于Brier分数）两种设计。
    *   **检查清单奖励**：引入一个由LLM作为评判员评估的加权检查清单，从可验证性、检索相关性、限定词完整性、实体关系明确性、无无根据添加等多个维度评估每个子声明的质量。
*   **组相对优势计算**：GRPO的核心创新在于其优势计算方式，通过将样本奖励与同批次（组）的平均奖励进行比较，缓解了不同复杂度输入间奖励分布差异带来的训练不稳定问题。
*   **联合优化目标**：最终奖励是上述三个奖励项的等权重和，使模型能同时优化输出格式合规性、与下游验证器的对齐程度以及分解本身的内在质量。

通过这一整合了结构化推理、知识蒸馏和多目标强化学习的框架，论文成功使较小的语言模型在声明验证任务上达到了最先进的性能。

### Q4: 论文做了哪些实验？

论文实验设置主要包括监督微调（SFT）和强化学习（RL）训练。SFT数据集基于长文本事实性基准（VeriScore和VeriFastScore）的开放模型响应构建，包含约1.57万句，经教师模型生成合成分解后，得到1.37万训练样本和1500测试样本。RL训练使用包含句子级事实性标注的数据集，保留原子性为1（单句）和2（多句跨度）的样本，共2300个训练实例，并以Wikipedia作为知识源。

评估在六个设置下进行，涵盖四个数据集：句子级评估使用FActScore的测试集（包含ChatGPT和PerplexityAI响应），按原子性-1和原子性-2两种粒度；响应级评估使用FELM（多领域世界知识）和BINGCHAT（长响应话题）数据集，均以Google Search为知识源。主要模型包括：分解器使用Llama-3.1-8B-Instruct，验证器使用Bespoke-MiniCheck-7B，教师模型为Llama-3.1-405B-Instruct，评估使用Llama-3.3-70B-Instruct。

对比方法包括：基于提示的基线（FActScore、VeriScore及零样本分解提示）、SFT基线（VeriScore提取器）、RL基线（DyDecomp），并探究了模型规模效应（使用Llama-3.3-70B和Llama-3.1-405B进行零样本提示）。评估指标包括平衡准确率（BAcc）、宏F1和子声明数量（SC）。

主要结果显示，本文方法（DAD）在8B模型上取得最佳验证性能，整体BAcc为71.37%，宏F1为71.75%，平均子声明数为8.14。具体数据指标上，在PerplexityAI原子性-1设置中，DAD的BAcc为79.70%、F1为73.15%；在ChatGPT原子性-2设置中，BAcc为81.60%、F1为78.55%。相比提示方法，宏F1提升1.99至6.24个百分点；相比现有RL方法（DyDecomp），提升5.84个百分点。人工评估证实生成的子声明在可验证性、连贯性等五个维度上质量较高。消融研究表明，结合SFT预热和多目标奖励（格式、验证器Brier分数、检查表）的完整框架效果最优，其中密集Brier奖励比稀疏奖励在PerplexityAI上F1提高1.85个百分点（72.71% vs. 70.86%），且学习更稳定高效。

### Q5: 有什么可以进一步探索的点？

本文提出的方法虽然在声明分解和验证方面取得了进步，但仍存在一些局限性，为未来研究提供了明确方向。首先，实验仅基于固定的验证器，且受限于现有参考数据集，这影响了模型的泛化能力。未来可探索框架在多种验证器和更广泛知识源上的适应性，例如结合动态检索增强生成（RAG）技术来实时接入外部知识库，以提升对复杂、新兴声明的处理能力。其次，监督预训练依赖于单一教师模型生成的合成样本，可能引入偏差并限制分解的多样性。后续工作可考虑集成多个不同规模的教师模型进行知识蒸馏，或引入对抗性训练来增强样本的多样性和鲁棒性。再者，尽管采用了多目标奖励和基于LLM的评估，这些自动化方法可能无法完全捕捉分解质量的细微差别；未来需要设计更精细的评估指标，并结合更广泛的人工评估来验证。最后，训练中使用的LLM评判机制可能引入潜在偏差，可通过对比不同大模型作为评判者的效果，或开发去偏差的奖励模型来进一步优化。总体而言，将分解与验证更紧密地结合，并探索更开放、动态的知识整合方式，是提升复杂声明验证性能的关键路径。

### Q6: 总结一下论文的主要内容

该论文针对复杂声明验证任务，提出了一种联合优化分解质量与验证器对齐的强化学习方法。核心问题是现有方法难以在子声明分解质量与下游验证性能之间取得有效对齐。为此，作者设计了基于Group Relative Policy Optimization (GRPO)的强化学习框架，该方法整合了三个关键部分：结构化顺序推理、基于教师模型蒸馏样本的监督微调，以及一个平衡格式合规性、验证器对齐和分解质量的多目标奖励函数。实验表明，所训练的80亿参数分解模型在六种评估设置中，将下游验证的宏观F1分数提升至71.75%，显著优于基于提示的方法和现有强化学习方法，并通过人工评估证实了生成子声明的高质量。该框架使得较小规模的语言模型能够通过联合优化验证准确性与分解质量，在声明验证任务上达到先进水平。
