---
title: "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization"
authors:
  - "Zeyuan Liu"
  - "Jeonghye Kim"
  - "Xufang Luo"
  - "Dongsheng Li"
  - "Yuqing Yang"
date: "2026-02-26"
arxiv_id: "2602.23008"
arxiv_url: "https://arxiv.org/abs/2602.23008"
pdf_url: "https://arxiv.org/pdf/2602.23008v1"
categories:
  - "cs.LG"
  - "cs.AI"
tags:
  - "Agent 架构"
  - "强化学习"
  - "记忆"
  - "探索"
  - "混合优化"
  - "泛化性"
relevance_score: 9.5
---

# Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization

## 原始摘要

Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决基于大语言模型（LLM）的智能体在强化学习训练中探索能力不足的核心瓶颈问题。研究背景是，尽管将LLM与强化学习结合，使其能通过与环境的交互和反馈来适应和学习，在交互决策、工具使用等 embodied AI 领域取得了进展，但现有方法主要依赖模型预训练阶段获得的知识，在已知分布内进行有限搜索，本质上是一种“利用”而非“探索”。当任务成功依赖于发现全新的环境状态或主动获取未知信息时，这类智能体往往表现不佳。

现有方法的不足主要体现在两方面：一是传统RL框架虽强调探索与利用的平衡，但LLM智能体系统过度依赖先验知识，缺乏系统性探索机制，导致在需要发现新状态的复杂环境中（如ScienceWorld和WebShop）容易陷入次优解。二是近期引入外部记忆模块作为长期记忆的方法，虽能利用过往经验修正错误，但由于参数固定，仅通过非参数更新收集的经验多样性有限，性能提升很快达到饱和，无法支持持续改进。

因此，本文要解决的核心问题是：如何设计一个统一的框架，使LLM智能体能够进行更广泛有效的探索，从而在需要多步推理和探索的复杂任务中持续学习并取得更优性能。为此，论文提出了EMPO²框架，其核心创新在于通过结合参数化策略的RL更新与非参数化记忆模块的交互更新，形成一种混合的在线与离线策略优化机制。该框架使智能体既能利用记忆辅助探索和学习，又能在没有记忆时保持鲁棒性，最终目标是减少对外部记忆的依赖，将探索能力内化到模型参数中，实现更强的适应性和泛化能力。

### Q2: 有哪些相关研究？

本文的相关研究主要可分为以下几类：

**1. 多步具身任务中的LLM智能体**：相关研究包括数据驱动方法（通过模仿学习增强决策）、基于模型的智能体（利用GPT-4等构建世界模型）以及利用模拟环境特权信息的方法。本文方法区别于这些工作，它减少对外部资源的依赖，强调通过智能体自主探索和自我改进来实现成长。

**2. LLM智能体的记忆机制**：例如Reflexion（存储语言反思用于后续提示）和REMEMBERER（记录观察、行动、奖励和Q值，并检索相似案例作为少样本示例）。这些方法表明LLM可以在不更新参数的情况下改进，但无法扩展内在知识，适应是短期且依赖外部记忆的。本文的EMPO²框架同样利用记忆进行探索，但通过结合策略上和策略下学习，旨在实现更长期的发展和泛化。

**3. 基于知识蒸馏的学习**：相关工作如上下文蒸馏，通过离线监督微调进行蒸馏。本文的混合策略下更新在在线训练中起到了奖励引导的知识蒸馏作用，但将其集成到在线强化学习中，从而同时利用在线适应性和增强探索以提高训练效率。

**4. 用于LLM智能体的强化学习**：相关工作包括从大型记录数据集中学习的离线RL（如Retrospex）和在线RL（如GiGPO，通过对类似观察的分组实现更精细的信用分配）。本文沿在线RL方向推进，通过将非参数记忆更新集成到策略上和策略下学习中，显著提高了样本效率。

**5. 在线RL的探索增强**：经典方法包括基于计数的探索、随机网络蒸馏以及Go-Explore（存储关键状态并重新探索）。其LLM扩展Intelligent Go-Explore在TextWorld等环境中表现强劲，但依赖大型闭源模型且不更新参数。另一项并行工作RLVMR使用热启动SFT来激发多样化的推理类型，并在在线RL中为每种类型提供密集的过程级奖励。本文的EMPO²同样致力于解决在线RL中的探索挑战，但通过混合优化框架和记忆机制来实现。

### Q3: 论文如何解决这个问题？

论文提出的EMPO²框架通过一种混合的在线与离线策略优化方法，结合了记忆增强机制，以解决大语言模型（LLM）智能体在强化学习中探索能力不足的问题。其核心思想是利用外部记忆存储智能体自我生成的反思提示（tips），并通过参数化（模型参数更新）与非参数化（外部记忆）两种更新方式的协同，引导智能体探索新状态并内化有效知识。

整体框架包含两个关键阶段：**交互阶段**和**更新阶段**。在交互阶段，智能体以概率 \(p\) 采用**记忆增强提示**模式，即从记忆缓冲区 \(\mathcal{M}\) 中检索与当前状态相关的tips来生成动作；以概率 \(1-p\) 采用**无记忆提示**模式，仅基于当前状态和任务生成动作。记忆中的tips并非来自独立模型，而是由策略 \(\pi_\theta\) 自身在每轮交互结束后，根据最终状态和特定提示自动生成的反思性总结，例如指出未完成的步骤或错误尝试，从而在后续交互中避免重复错误并探索新策略。

在更新阶段，对于记忆增强模式下产生的轨迹，框架进一步引入两种更新模式：以概率 \(1-q\) 进行**在线策略更新**，即保留tips条件计算重要性采样比；以概率 \(q\) 进行**离线策略更新**，即在更新时移除tips条件，仅基于无记忆的状态计算动作概率。这种设计使得高奖励的轨迹被强化，低奖励的被抑制，实现了**基于奖励的知识蒸馏**：tips作为临时的“脚手架”机制提升探索和轨迹质量，而奖励信号确保只有有益行为最终被内化到策略参数中，从而使训练后的策略在推理时无需依赖记忆也能保持鲁棒性能。

此外，论文还引入了两项关键技术以提升训练稳定性与探索效率：一是**掩码机制**，在离线策略更新中过滤掉策略概率低于阈值 \(\delta\) 的token，防止低概率token导致梯度爆炸；二是**内在奖励**，基于状态新颖性（如与记忆状态的余弦相似度）给予额外奖励，激励智能体探索未见状态，并维持策略熵，避免过早收敛。

创新点主要体现在：1) **混合策略优化**：首次将在线与离线更新结合，并协调参数化与非参数化学习；2) **自我生成与利用记忆**：策略自身生成反思性tips并用于条件生成，形成自主探索循环；3) **奖励引导的知识内化**：通过离线策略更新将记忆辅助下的高质量行为蒸馏到无记忆策略中，兼顾探索性能与推理时鲁棒性。实验表明，该方法在ScienceWorld和WebShop任务上显著优于基线，并在分布外任务上展现出强大的适应能力。

### Q4: 论文做了哪些实验？

论文在 ScienceWorld 和 WebShop 两个广泛使用的 LLM 智能体基准测试上进行了实验，以 Qwen2.5-7B-Instruct 作为基础模型。实验设置上，作者基于 verl 库实现了 EMPO²，将 GRPO 从单步扩展到多步设置，并加入了记忆模块和离策略损失计算组件，与 GRPO 使用相同的超参数配置。

在 ScienceWorld 基准上，实验覆盖了化学、分类、生物学、电学和测量等主题下的 19 个任务。每个任务包含多个变体，使用前 5 个变体进行训练，并在 20 个未见过的测试变体上进行评估。对比方法包括：非参数化 RL 方法 Reflexion、离线 RL 方法 Retrospex（作者将其基础模型统一为 Qwen2.5-7B-Instruct 并移除了启发式方法）以及在线 RL 方法 GRPO。

主要结果显示，在 ScienceWorld 上，EMPO² 的平均回报为 75.9，显著优于所有基线（Naive: -61.3, Reflexion: 17.1, Retrospex: 33.8, GRPO: 33.2）。具体而言，EMPO² 在多个任务上达到了满分 100 分，其平均性能相比 GRPO 提升了一倍以上（相对提升 128.6%）。在 WebShop 基准上，EMPO² 相比 GRPO 也取得了 11.3% 的性能提升。此外，在分布外测试中，EMPO² 展示了更强的任务适应能力，仅需少量带有记忆的试验且无需参数更新，就能在新任务上快速适应。

### Q5: 有什么可以进一步探索的点？

该论文提出的EMPO²框架在探索能力和泛化性上取得了显著进展，但其局限性和未来研究方向仍值得深入探讨。首先，其探索过程高度依赖记忆模块，在完全未知或记忆无法有效检索的环境中，探索效率可能下降。其次，框架中的混合策略优化虽然平衡了样本效率与稳定性，但计算开销较大，且对超参数（如记忆检索权重、策略更新比例）较为敏感，这限制了其在大规模复杂环境中的可扩展性。

未来研究可从以下几个方向展开：一是探索更高效、可解释的记忆机制，例如引入分层记忆或基于注意力的动态记忆检索，以降低对精确匹配的依赖；二是研究无监督或自监督的探索辅助方法，如通过预测环境变化或构建内在奖励来激励智能体发现新颖状态，减少对预训练知识的偏置；三是将框架扩展至多模态或具身任务，测试其在物理交互或跨模态理解中的泛化能力；四是优化训练效率，尝试分布式学习或元学习策略，使智能体能快速适应新任务而无需完整微调。这些改进有望进一步提升LLM智能体的自主探索与终身学习能力。

### Q6: 总结一下论文的主要内容

本文提出了一种名为EMPO²的混合强化学习框架，旨在解决大型语言模型（LLM）智能体在强化学习中探索能力不足的核心瓶颈。现有方法主要依赖预训练知识，难以在需要发现新状态的环境中有效探索。EMPO²通过结合参数化策略更新与非参数化记忆模块，构建了一个统一的框架。其核心方法是在智能体与环境交互的rollout阶段，设计了使用记忆和不使用记忆两种模式；在策略更新阶段，则融合了on-policy和off-policy学习。这种混合机制使得智能体既能利用记忆进行有效探索，又能在没有记忆时保持鲁棒性能。实验在ScienceWorld和WebShop两个多步推理环境上进行，结果表明EMPO²显著优于现有基线，性能分别提升了128.6%和11.3%。更重要的是，在分布外测试中，仅需少量试验且无需参数更新，智能体就能适应新任务，证明了该框架在提升LLM智能体探索性与泛化能力方面的有效性和潜力。
