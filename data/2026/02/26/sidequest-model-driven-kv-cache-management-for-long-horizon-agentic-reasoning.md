---
title: "SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning"
authors:
  - "Sanjay Kariyappa"
  - "G. Edward Suh"
date: "2026-02-26"
arxiv_id: "2602.22603"
arxiv_url: "https://arxiv.org/abs/2602.22603"
pdf_url: "https://arxiv.org/pdf/2602.22603v1"
categories:
  - "cs.AI"
  - "cs.LG"
tags:
  - "Agent 架构"
  - "Agent 推理"
  - "KV Cache 管理"
  - "长程推理"
  - "模型驱动优化"
  - "计算效率"
relevance_score: 9.0
---

# SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning

## 原始摘要

Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在解决长程智能体推理任务中键值（KV）缓存管理效率低下的问题。随着大语言模型从静态对话接口演变为自主智能体，其应用场景扩展到深度研究、自动化软件工程等需要多跳推理的长时任务。在此类任务中，模型需综合处理来自多个检索文档和中间思考痕迹的大量信息，导致上下文长度急剧增长，KV缓存随之线性膨胀，成为推理效率的主要瓶颈：它不仅占用大量GPU内存，限制批处理规模和并发请求数，还使注意力机制受限于内存带宽，严重影响解码性能。

现有方法主要依赖固定启发式规则进行KV缓存压缩，例如基于历史注意力分数保留重要令牌或局部窗口内聚类信息。这些方法假设令牌的历史重要性可预测其未来相关性，适用于处理固定长文本（如书籍、法律文档）的查询场景。然而，在智能体推理这类动态任务中，上下文随工具调用和内部反思不断演化，令牌的效用呈现动态非单调特性——早期低重要性令牌可能在后续合成步骤中变得关键。现有启发式方法因无法捕捉这种动态性，常导致重要信息被过早修剪，引发难以调试的推理失败。

本文的核心问题是：如何为动态演化的长程智能体任务设计高效的KV缓存管理机制，以在保证推理准确性的同时显著降低内存占用。为此，论文提出SideQuest框架，将KV缓存管理从固定启发式转向智能的模型驱动过程。其核心思路是让大推理模型自身基于对任务状态的语义理解，主动执行KV缓存清理，实现“记忆垃圾回收”。为避免管理过程干扰主任务，该框架将缓存压缩设计为与主推理线程并行执行的辅助任务，从而在保持精简高效缓存的同时，防止管理令牌污染主要注意力窗口。通过仅用215个样本训练的模型，SideQuest在智能体任务中实现了高达65%的峰值令牌使用减少，且准确性损失极小，显著优于基于启发式的压缩技术。

### Q2: 有哪些相关研究？

本文的相关研究主要围绕缓解长上下文推理的计算与内存开销展开，可分为以下几类：

**1. 架构创新类**：如分组查询注意力（GQA）和多头潜在注意力（MLA），通过共享键值头来降低每令牌存储成本；以及Mamba、门控Delta层等替代标准注意力层的方案，旨在实现次线性内存增长。这些方法从模型设计层面减少KV状态固有大小，但并未解决多轮智能体推理中历史上下文累积的管理问题。

**2. 稀疏注意力类**：以DeepSeek稀疏注意力（DSA）为代表，通过动态选择相关令牌子集进行计算，减轻内存带宽压力，但通常不减少KV缓存的物理存储需求，完整上下文仍驻留内存。

**3. KV缓存淘汰技术类**：与本文工作最为接近。StreamingLLM和H₂O等采用静态启发式策略（如保留注意力汇聚点或基于累积注意力得分的高频令牌）；SnapKV通过识别注意力窗口内的关键信息簇进行优化。近期研究开始针对推理模型进行扩展：R-KV基于冗余性丢弃推理令牌；RaaS提出关键里程碑令牌的概念并优先保留；LazyEviction选择性保留推理中反复重要的思维令牌；ThinKV采用分层分类方法，根据重要性进行选择性量化和剪枝。

**本文与这些工作的关系与区别**：SideQuest同样属于KV缓存管理范畴，但针对现有方法的局限性进行了创新。现有方法（尤其是推理导向技术）主要面向静态上下文的单步思维链任务，依赖于预定义启发式规则，缺乏语义感知能力。在智能体任务中，令牌重要性动态变化，静态启发式可能错误剪裁关键信息。本文提出的SideQuest则利用大型推理模型自身对上下文令牌的有用性进行推理，实现语义感知的KV压缩，并将其构建为与主推理任务并行的辅助任务，避免了管理过程污染模型内存。因此，本文在动态、多步的智能体推理场景中，相比基于启发式的方法具有显著优势。

### Q3: 论文如何解决这个问题？

论文通过提出SideQuest这一模型驱动的KV缓存管理框架，来解决长视野智能体推理任务中因外部检索内容过多导致内存占用激增、解码性能下降的问题。其核心思路是利用大型推理模型自身的推理能力，动态判断上下文中哪些工具调用及响应已失效并予以清除，而非依赖基于注意力的启发式方法。

整体框架与主要模块设计如下：SideQuest被集成在ReAct框架中，采用并行执行的架构。系统包含一个主线程执行常规的ReAct推理循环（思考、工具调用、接收响应），同时周期性地分叉出一个辅助线程。该辅助线程与主线程共享相同的上下文，但被专门引导去执行缓存管理任务。其工作流程分为四个步骤：1) 在固定间隔触发并行执行，创建辅助线程；2) 辅助线程进行“陈旧性推理”，模型分析当前所有已打开的工具输出，判断哪些已不再需要；3) 辅助线程输出结构化的删除命令；4) 系统同步，等待主线程当前轮次完成后，再根据命令从KV缓存中清除被标记的工具响应。

关键技术包括：1) **并行架构**：将管理任务隔离在独立的辅助线程中，避免了管理令牌污染主线程上下文，也减少了主任务延迟。2) **双管齐下的引导策略**：通过“上下文触发器”（在辅助线程上下文前添加特定提示短语，如“** Memory management mode **”）和**任务特定的微调**，使模型能切换至管理子程序，专注于分析工具响应的效用。3) **混合训练方法**：使用包含两种数据的混合数据集进行训练。一是**主轨迹数据**，通过从原始策略提取的logits进行蒸馏损失训练，以保持模型原有的推理能力；二是**辅助轨迹数据**，通过后见之明标注生成，模拟部分缓存已被压缩的场景，训练模型识别过期游标并生成删除命令。训练采用联合优化目标，结合了辅助数据的交叉熵损失和主数据的蒸馏损失。

创新点在于：首次提出由模型自身进行语义感知的KV缓存管理，通过并行辅助线程实现无干扰的实时压缩；设计了高效的触发与训练机制，使单一模型能无缝切换于主任务推理与缓存管理之间；仅需少量样本（215个）进行训练，即可在智能体任务上显著降低峰值令牌使用量（高达65%），且精度损失极小，优于基于启发式的方法。

### Q4: 论文做了哪些实验？

论文在长上下文、多轮网页浏览任务上进行了实验，主要评估SideQuest在KV缓存压缩方面的效果。实验设置方面，使用gpt-oss-20b模型，并基于SGLang框架实现压缩技术以支持高效的上下文重用。训练数据来自FRAMES数据集，通过筛选和标注流程获得了215个主轨迹和1274个辅助轨迹样本，使用LoRA进行微调。

使用的数据集/基准测试包括：1) FRAMES数据集，评估模型在维基百科文章上进行检索和多跳推理的能力，使用了包含640万篇文章的语料库，并在424个样本上报告指标；2) BrowseComp数据集，测试在网络上查找特定信息的能力，使用了包含10万份文档的固定语料库，在500个样本子集上报告结果。

对比方法包括：未压缩基线（完全注意力）以及三种启发式KV缓存压缩技术：H₂O（基于累积注意力得分保留重要标记）、SnapKV（保留注意力窗口内的关键信息簇）和R-KV（基于冗余度对标记评分并剪枝）。所有启发式基线在16k和24k两种令牌预算下进行评估。

主要结果如下：在效率与效用权衡方面，SideQuest相比未压缩基线，峰值令牌使用量减少了56-65%，KV缓存内存读取量减少了53-71%，而推理性能损失很小：在FRAMES基准上准确率仅下降最多2%，在BrowseComp基准上下降5%。相比之下，启发式基线在类似压缩水平下准确率大幅下降。在服务性能方面，在FRAMES基准上，SideQuest将峰值系统吞吐量提高了83.9%（从828 tok/s提升至1523 tok/s），峰值KV缓存使用量降低了53.9%（归一化占用从0.977降至0.450），总基准运行时间减少了36.8%（从2356秒降至1489秒）。此外，SideQuest的非完成率与未压缩基线相当，远低于启发式方法，显示出更好的鲁棒性和连贯性。

### Q5: 有什么可以进一步探索的点？

该论文的局限性在于其评估主要集中于多轮网页浏览任务，未来可探索将其应用于代码助手等需要处理大型代码库和长上下文依赖图的领域，以验证其通用性。此外，SideQuest 架构虽用于内存管理，但其并行执行辅助任务的机制可扩展至安全监管、内容审核等任务，替代现有独立的护栏模型，从而利用主模型的全上下文感知能力提升效率。可能的改进方向包括：优化辅助任务的训练样本效率，当前仅用 215 个样本训练，可研究如何通过合成数据或自监督学习进一步降低数据需求；探索动态调整压缩阈值的机制，使模型能根据任务复杂度自适应管理缓存；以及将 SideQuest 与硬件级优化结合，实现端到端的推理加速。这些方向有望推动长程推理代理在资源受限环境中的实用化。

### Q6: 总结一下论文的主要内容

这篇论文针对长程智能体任务中因外部检索信息导致KV缓存快速增长、影响解码性能的问题，提出了SideQuest框架。其核心贡献在于摒弃了静态启发式压缩方法，转而利用大型推理模型自身来动态管理内存。该方法将KV缓存压缩定义为一个与主推理任务并行的辅助任务，使模型能够基于语义理解判断上下文中各令牌的效用，从而精准淘汰过时或无用的工具输出信息。实验表明，仅用215个样本训练的模型即可实现该方法，在智能体任务中能将峰值令牌使用量降低高达65%，且准确率损失极小，显著优于基于启发式的基线方法。SideQuest的意义在于将内存管理从固定约束转变为可学习的推理技能，为高效的长上下文智能体推理建立了新范式。
