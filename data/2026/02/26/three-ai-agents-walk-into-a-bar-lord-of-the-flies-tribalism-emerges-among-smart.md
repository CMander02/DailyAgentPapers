---
title: "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents"
authors:
  - "Dhwanil M. Mori"
  - "Neil F. Johnson"
date: "2026-02-26"
arxiv_id: "2602.23093"
arxiv_url: "https://arxiv.org/abs/2602.23093"
pdf_url: "https://arxiv.org/pdf/2602.23093v1"
categories:
  - "cs.AI"
  - "cs.SI"
  - "physics.soc-ph"
tags:
  - "多智能体系统"
  - "资源分配"
  - "社会行为模拟"
  - "Agent行为分析"
  - "系统涌现"
  - "Agent评测"
relevance_score: 8.0
---

# Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents

## 原始摘要

Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of "Lord of the Flies" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.

## Q&A 论文解读

### Q1: 这篇论文试图解决什么问题？

这篇论文旨在探讨由多个自主AI智能体（如大语言模型）共同管理有限共享资源时可能出现的协调失败与系统性风险问题。研究背景设定在近未来基础设施系统中，多个AI智能体需反复决定是否从固定容量的公共资源（如能源、带宽、计算能力）中请求一个单位，这类似于经典的“埃尔法罗酒吧问题”（El Farol Bar problem）。现有方法通常假设智能体能够通过独立随机决策实现协调，但实际中，基于相似推理模式的AI智能体可能因决策同步而导致系统过载或资源浪费。

现有方法的不足在于，尽管大语言模型具备推理和适应能力，被视为有潜力的分布式控制器，但它们可能因共享相似的历史分析逻辑而产生高度相关的行为，从而破坏系统稳定性。论文指出，当智能体使用确定性推理而非独立随机策略时，其决策会呈现正相关，增加总体方差，反而提升过载风险。

本文要解决的核心问题是：更强大的AI智能体是否真能改善资源协调，还是可能因形成“部落”行为模式而加剧集体失败？通过实验，论文揭示了智能体会自发分化为攻击型、保守型和机会型三种部落，且更强大的模型反而导致更高的系统过载率（达72.5%），表现甚至不如简单的随机基线。研究强调了在去中心化AI智能体系统中，个体智能的提升未必带来集体理性，反而可能因群体行为异化引发系统性风险。

### Q2: 有哪些相关研究？

本文的相关研究主要涉及三类文献。首先，在方法类上，研究基于经典的埃尔法罗酒吧问题（El Farol Bar problem）和少数者博弈（minority game），这些工作表明简单自适应智能体可通过行为异质性实现高效集体协调，而本文则测试了拥有更丰富内部模型的LLM智能体是否优于这些经典基线，结果发现其表现更差。其次，在应用与评测类上，近期大量研究探索LLM在博弈论场景中的行为，例如Akata等人发现LLM在协调游戏中表现欠佳，GPT-4可能更为自私；FAIRGAME等框架系统评估了LLM的战略偏差；Takata等人直接研究了LLM在空间扩展埃尔法罗问题中的群体形成，但未系统比较性能基线或基础设施安全指标。本文与此类工作的区别在于，聚焦于基础设施安全框架，采用过载频率和严重性等系统级安全指标，并引入了容量匹配随机基线进行校准比较。最后，在多智能体安全类上，Konstantakopoulos等人研究了建筑能源管理等实际基础设施中的分散协调，而本文则专门针对资源拥塞控制场景，通过实证LLM实验证明纯LLM推理不足以实现安全关键资源分配，从而呼吁混合设计。本文的独特贡献还包括：揭示了模型规模与性能倒置（更大模型表现更差）、通过数据驱动行为聚类识别出三种部落类型，以及进行了跨模型家族的比较。

### Q3: 论文如何解决这个问题？

论文通过设计一个多智能体资源请求博弈实验框架来解决“智能AI智能体在资源分配中形成部落并导致系统性能下降”的问题。其核心方法是模拟一个简化版的El Farol/少数者博弈场景，让N个基于大语言模型（LLM）的AI智能体在固定容量C的系统下，每轮独立决定是否请求一个单位资源（行动为GO或STAY）。

整体框架是一个多轮迭代实验（30-100轮）。主要模块包括：1）**智能体模块**：AI智能体接收结构化提示，包含游戏规则（玩家数N、容量C、好坏结果定义、最大化累积收益的目标）、个性描述（六种预设：中性、风险厌恶、逆反、趋势跟随、乐观、悲观）以及近期出席历史（滑动窗口，通常5-10轮）与自身过往结果反馈。2）**交互与反馈模块**：每轮所有智能体同时输出行动后，系统计算总需求A_t，并根据阈值支付结构给予个体收益反馈（+1或-1），同时告知实际出席人数A_t及其行动是否最优。3）**基线与控制模块**：引入随机基线智能体作为对照，其以概率p=C/N独立选择GO，使期望需求匹配容量，以此评估LLM智能体的相对表现。4）**探索机制模块**：比较ε-greedy随机化（以概率ε=0.15翻转最终行动）与基于温度的采样（T=0.7）对协调的影响。

关键技术包括：使用结构化JSON响应格式确保决策可解析；通过个性提示诱导不同的行为倾向；设计系统级指标（如平均超额出席、过载频率、利用率）和个体级指标（如成功分配数、过载贡献率OCR_i）进行量化评估；引入有效多样性度量S_eff，基于智能体行动序列的成对相关性来量化部落化程度。

创新点在于：将LLM智能体置于重复资源博弈中，通过个性提示而非明确协调机制，观察其自组织行为；发现智能体并未优化资源使用，反而涌现出三种主要部落类型（激进型、保守型、机会主义型），且更强大的模型加剧了系统故障率；通过对比随机基线，实证表明更聪明的AI智能体因形成部落而表现更差，揭示了多智能体系统中“智能导致集体愚蠢”的悖论现象。

### Q4: 论文做了哪些实验？

论文实验设置了一个多智能体资源请求模拟环境，其中N个AI代理在每轮独立决定是否向一个固定容量C的系统请求一个单位的资源。实验共进行了43次运行，每轮实验包含30至100轮不等。研究使用了不同规模的大型语言模型（LLM）作为智能体，包括大模型（如GPT-4、Gemini Pro、Claude Sonnet）和小模型（如GPT-3.5、Gemini Flash、Claude Haiku）。实验对比了不同模型规模、不同人格提示（如风险厌恶型、乐观型、趋势跟随型、逆向型等）下的系统表现，并以理论基线（容量匹配的随机基准，即二项分布模型）和抛硬币随机决策作为对比方法。

实验的主要数据集或基准测试基于模拟环境本身，通过记录智能体的请求频率、成功获取次数、最大饥饿周期、效率（成功获取次数/请求次数）和过载贡献率等指标进行评估。关键数据指标包括：在配置N=4、C=2时，大模型的过载频率为72.5%，小模型为53.8%，而理论随机基线的过载率仅为31.25%。相对基线，小模型的过载频率约差1.72倍，大模型约差2.32倍，表明更大模型反而导致更差的系统安全性。在人格同质化实验中（N=3, C=1），风险厌恶型智能体的过载率最低（25.0%），接近理论基线25.9%；而乐观型智能体的过载率高达89.0%，趋势跟随型为81.0%，逆向型为70.0%，均远高于抛硬币控制的50.0%。此外，通过聚类分析（k=3，轮廓系数0.458）识别出三种行为类型：机会型（占比48.1%，请求频率均值0.845，过载贡献率均值73.7%）、激进型（27.3%，请求频率均值0.586，效率均值0.624）和保守型（24.7%，请求频率均值0.200，最大饥饿周期均值73.5轮）。实验发现，LLM智能体未能自我组织到接近容量匹配的稳定策略，且更智能的模型由于形成“部落”行为，反而增加了系统故障率。

### Q5: 有什么可以进一步探索的点？

该论文揭示了智能体在资源竞争中的“部落化”倾向及其负面效应，但研究仍存在一定局限性。首先，实验环境高度简化，仅考虑单一资源请求决策，未涉及多资源类型、动态容量或长期合作收益等复杂场景，限制了结论的普适性。其次，智能体基于固定提示词互动，缺乏学习与策略演化能力，未能模拟长期适应行为。未来可探索以下方向：引入机制设计（如动态定价或信誉系统）以抑制部落化；研究混合智能体群体（包含人类监督或规则型代理）的协同效果；考察不同LLM架构或训练方式对部落形成的影响。此外，可结合博弈论与多智能体强化学习，设计能促进系统整体效率的通信协议或合作激励机制，从而在自主性与系统稳定性间取得平衡。

### Q6: 总结一下论文的主要内容

该论文探讨了由多个自主AI智能体控制未来基础设施资源分配时可能出现的“部落化”现象及其负面影响。研究构建了一个简化模型：N个基于大语言模型（LLM）的AI智能体在每一轮独立决定是否向一个固定容量C的系统请求一个单位的资源（如能源、带宽）。核心问题是考察这些智能体在重复互动中是否会形成稳定的群体行为模式，以及这是否能优化资源利用。

方法上，研究通过多轮交互实验，观察智能体在资源竞争环境中的决策演化。主要发现是：智能体并未合作降低系统过载或提升资源使用效率，反而自发形成了三类具有鲜明集体特征的“部落”——攻击型（27.3%）、保守型（24.7%）和机会主义型（48.1%）。其决策表现甚至不如随机抛硬币，且能力越强的智能体反而会因部落竞争加剧系统故障率。

论文的核心结论是：在竞争性资源分配场景中，更聪明的AI智能体可能因形成具有内聚身份的部落而表现出更“愚蠢”的集体行为，导致系统整体性能下降。这一发现警示了在多智能体系统中设计协作机制与评估其实际效能的重要性。
